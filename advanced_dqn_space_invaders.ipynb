{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/tcharos/AIDL_B02-Advanced-Topics-in-Deep-Learning/blob/main/advanced_dqn_space_invaders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced DQN Variants for Space Invaders\n",
    "\n",
    "Implementation of DQN, Double DQN, Dueling DQN, and Prioritized Experience Replay for ALE/SpaceInvaders-v5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: gymnasium[atari,accept-rom-license]\n",
      "Requirement already satisfied: ale-py in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (0.11.2)\n",
      "Requirement already satisfied: numpy>1.20 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from ale-py) (1.26.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.10.0 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from ale-py) (7.1.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from ale-py) (4.11.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from importlib-metadata>=4.10.0->ale-py) (3.18.1)\n",
      "Requirement already satisfied: torch in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (2.8.0)\n",
      "Requirement already satisfied: scipy in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (1.13.0)\n",
      "Requirement already satisfied: numpy in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (1.26.4)\n",
      "Requirement already satisfied: psutil in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (5.9.8)\n",
      "Requirement already satisfied: filelock in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from jinja2->torch) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium[atari,accept-rom-license]\n",
    "!pip install ale-py\n",
    "!pip install torch scipy numpy psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and verify ALE is available\n",
    "import ale_py\n",
    "import gymnasium as gym\n",
    "gym.register_envs(ale_py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "from scipy.ndimage import zoom\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Register ALE environments\n",
    "import ale_py\n",
    "gym.register_envs(ale_py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Drive Setup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoints will be saved locally to: ./checkpoints\n",
      "Best models will be saved locally to: ./best_models\n"
     ]
    }
   ],
   "source": [
    "USE_GDRIVE = False  # Set to True to enable Google Drive integration\n",
    "\n",
    "if USE_GDRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    CHECKPOINT_DIR = '/content/drive/MyDrive/DQN_SpaceInvaders_Checkpoints'\n",
    "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "    print(f\"Checkpoints will be saved to: {CHECKPOINT_DIR}\")\n",
    "else:\n",
    "    CHECKPOINT_DIR = './checkpoints'\n",
    "    BEST_MODELS = './best_models'\n",
    "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "    os.makedirs(BEST_MODELS, exist_ok=True)\n",
    "    print(f\"Checkpoints will be saved locally to: {CHECKPOINT_DIR}\")\n",
    "    print(f\"Best models will be saved locally to: {BEST_MODELS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Network Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"Standard DQN Network\"\"\"\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "    \n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(conv_out)\n",
    "\n",
    "\n",
    "class DuelingDQN(nn.Module):\n",
    "    \"\"\"Dueling DQN Network with separate value and advantage streams\"\"\"\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        \n",
    "        # Value stream\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "        \n",
    "        # Advantage stream\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "    \n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        value = self.value_stream(conv_out)\n",
    "        advantage = self.advantage_stream(conv_out)\n",
    "        \n",
    "        # Combine value and advantage using the aggregation formula\n",
    "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Standard Experience Replay Buffer\"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = zip(*batch)\n",
    "        return np.array(state), action, reward, np.array(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    \"\"\"Prioritized Experience Replay Buffer\"\"\"\n",
    "    def __init__(self, capacity, alpha=0.6, beta_start=0.4, beta_frames=100000):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha  # How much prioritization to use (0 = uniform, 1 = full prioritization)\n",
    "        self.beta_start = beta_start  # Importance sampling weight\n",
    "        self.beta_frames = beta_frames\n",
    "        self.frame = 1\n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros(capacity, dtype=np.float32)\n",
    "        self.pos = 0\n",
    "    \n",
    "    def beta_by_frame(self, frame_idx):\n",
    "        \"\"\"Linearly increase beta from beta_start to 1.0\"\"\"\n",
    "        return min(1.0, self.beta_start + frame_idx * (1.0 - self.beta_start) / self.beta_frames)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        max_priority = self.priorities.max() if self.buffer else 1.0\n",
    "        \n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append((state, action, reward, next_state, done))\n",
    "        else:\n",
    "            self.buffer[self.pos] = (state, action, reward, next_state, done)\n",
    "        \n",
    "        self.priorities[self.pos] = max_priority\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        N = len(self.buffer)\n",
    "        if N == self.capacity:\n",
    "            priorities = self.priorities\n",
    "        else:\n",
    "            priorities = self.priorities[:self.pos]\n",
    "        \n",
    "        # Calculate sampling probabilities\n",
    "        probabilities = priorities ** self.alpha\n",
    "        probabilities /= probabilities.sum()\n",
    "        \n",
    "        # Sample indices based on priorities\n",
    "        indices = np.random.choice(N, batch_size, p=probabilities, replace=False)\n",
    "        \n",
    "        # Calculate importance sampling weights\n",
    "        beta = self.beta_by_frame(self.frame)\n",
    "        self.frame += 1\n",
    "        \n",
    "        weights = (N * probabilities[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        \n",
    "        # Get samples\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        state, action, reward, next_state, done = zip(*samples)\n",
    "        \n",
    "        return np.array(state), action, reward, np.array(next_state), done, indices, weights\n",
    "    \n",
    "    def update_priorities(self, indices, priorities):\n",
    "        \"\"\"Update priorities of sampled transitions\"\"\"\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            self.priorities[idx] = priority\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frame Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    \"\"\"\n",
    "    Standard DQN preprocessing: \n",
    "    1. Grayscale\n",
    "    2. Resize/Crop to 84x84\n",
    "    3. Normalize\n",
    "    \"\"\"\n",
    "    # 1. Convert to grayscale (The frame should be H x W x 3)\n",
    "    # Use standard cv2 conversion for simplicity\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # 2. Crop/Resize to 84x84\n",
    "    # Standard practice is to crop to the relevant game area, then resize.\n",
    "    # A simplified, but effective, approach is direct resizing:\n",
    "    # Use INTER_AREA for downsampling (best quality)\n",
    "    resized_frame = cv2.resize(\n",
    "        gray_frame, \n",
    "        (84, 84), \n",
    "        interpolation=cv2.INTER_AREA\n",
    "    )\n",
    "\n",
    "    # 3. Normalize\n",
    "    normalized_frame = resized_frame.astype(np.float32) / 255.0\n",
    "    \n",
    "    # DQN expects a channel dimension, even if it's 1, often added during stacking\n",
    "    return normalized_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, epsilon, policy_net, n_actions, device):\n",
    "    \"\"\"Epsilon-greedy action selection\"\"\"\n",
    "    if random.random() < epsilon:\n",
    "        return random.randrange(n_actions)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            q_values = policy_net(state_tensor)\n",
    "            return q_values.max(1)[1].item()\n",
    "\n",
    "\n",
    "def optimize_model_dqn(policy_net, target_net, optimizer, replay_buffer, batch_size, gamma, device):\n",
    "    \"\"\"Standard DQN optimization\"\"\"\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return None\n",
    "    \n",
    "    states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "    \n",
    "    states = torch.FloatTensor(states).to(device)\n",
    "    actions = torch.LongTensor(actions).to(device)\n",
    "    rewards = torch.FloatTensor(rewards).to(device)\n",
    "    next_states = torch.FloatTensor(next_states).to(device)\n",
    "    dones = torch.FloatTensor(dones).to(device)\n",
    "    \n",
    "    # Current Q values\n",
    "    current_q = policy_net(states).gather(1, actions.unsqueeze(1))\n",
    "    \n",
    "    # Next Q values from target network\n",
    "    next_q = target_net(next_states).max(1)[0].detach()\n",
    "    target_q = rewards + (1 - dones) * gamma * next_q\n",
    "    \n",
    "    # Loss\n",
    "    loss = F.mse_loss(current_q.squeeze(), target_q)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def optimize_model_double_dqn(policy_net, target_net, optimizer, replay_buffer, batch_size, gamma, device):\n",
    "    \"\"\"Double DQN optimization\"\"\"\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return None\n",
    "    \n",
    "    states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "    \n",
    "    states = torch.FloatTensor(states).to(device)\n",
    "    actions = torch.LongTensor(actions).to(device)\n",
    "    rewards = torch.FloatTensor(rewards).to(device)\n",
    "    next_states = torch.FloatTensor(next_states).to(device)\n",
    "    dones = torch.FloatTensor(dones).to(device)\n",
    "    \n",
    "    # Current Q values\n",
    "    current_q = policy_net(states).gather(1, actions.unsqueeze(1))\n",
    "    \n",
    "    # Double DQN: use policy net to select actions, target net to evaluate them\n",
    "    with torch.no_grad():\n",
    "        next_actions = policy_net(next_states).max(1)[1].unsqueeze(1)\n",
    "        next_q = target_net(next_states).gather(1, next_actions).squeeze()\n",
    "    \n",
    "    target_q = rewards + (1 - dones) * gamma * next_q\n",
    "    \n",
    "    # Loss\n",
    "    loss = F.mse_loss(current_q.squeeze(), target_q)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def optimize_model_per(policy_net, target_net, optimizer, replay_buffer, batch_size, gamma, device, dqn_type='DQN'):\n",
    "    \"\"\"Optimization with Prioritized Experience Replay\"\"\"\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return None\n",
    "    \n",
    "    states, actions, rewards, next_states, dones, indices, weights = replay_buffer.sample(batch_size)\n",
    "    \n",
    "    states = torch.FloatTensor(states).to(device)\n",
    "    actions = torch.LongTensor(actions).to(device)\n",
    "    rewards = torch.FloatTensor(rewards).to(device)\n",
    "    next_states = torch.FloatTensor(next_states).to(device)\n",
    "    dones = torch.FloatTensor(dones).to(device)\n",
    "    weights = torch.FloatTensor(weights).to(device)\n",
    "    \n",
    "    # Current Q values\n",
    "    current_q = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
    "    \n",
    "    # Calculate target Q values based on DQN type\n",
    "    with torch.no_grad():\n",
    "        if dqn_type == 'DoubleDQN':\n",
    "            next_actions = policy_net(next_states).max(1)[1].unsqueeze(1)\n",
    "            next_q = target_net(next_states).gather(1, next_actions).squeeze()\n",
    "        else:  # Standard DQN or Dueling DQN\n",
    "            next_q = target_net(next_states).max(1)[0]\n",
    "        \n",
    "        target_q = rewards + (1 - dones) * gamma * next_q\n",
    "    \n",
    "    # Calculate TD errors for priority update\n",
    "    td_errors = torch.abs(current_q - target_q).detach().cpu().numpy()\n",
    "    \n",
    "    # Weighted loss\n",
    "    loss = (weights * F.mse_loss(current_q, target_q, reduction='none')).mean()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Update priorities\n",
    "    new_priorities = td_errors + 1e-6  # Add small epsilon to prevent zero priority\n",
    "    replay_buffer.update_priorities(indices, new_priorities)\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def print_config(config):\n",
    "    \"\"\"Print configuration in a formatted way\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"  DQN TYPE: {config['DQN_TYPE']}\")\n",
    "    if config['USE_PER']:\n",
    "        print(f\"  Using Prioritized Experience Replay (PER)\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nConfiguration:\")\n",
    "    print(\"-\"*70)\n",
    "    for key, value in config.items():\n",
    "        if key not in ['CHECKPOINT_DIR']:  # Skip long paths\n",
    "            print(f\"  {key:20s}: {value}\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "\n",
    "def save_checkpoint(config, policy_net, target_net, optimizer, episode, avg_score, \n",
    "                   episode_rewards, is_best=False):\n",
    "    \"\"\"Save model checkpoint\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    dqn_type = config['DQN_TYPE']\n",
    "    per_suffix = \"_PER\" if config['USE_PER'] else \"\"\n",
    "    \n",
    "    if is_best:\n",
    "        filename = f\"{dqn_type}{per_suffix}_{timestamp}_best.pth\"\n",
    "        filepath = os.path.join(config['BEST_MODELS_DIR'], filename)\n",
    "        print(f'\\t\\tNew best avg: {avg_score:.2f} - saved to {filename}')\n",
    "    else:\n",
    "        filename = f\"{dqn_type}{per_suffix}_ep{episode}_{timestamp}.pth\"\n",
    "        filepath = os.path.join(config['CHECKPOINT_DIR'], filename)\n",
    "        print(f'\\t\\tCheckpoint saved: {filename}')\n",
    "\n",
    "    torch.save({\n",
    "        'episode': episode,\n",
    "        'config': config,\n",
    "        'policy_net_state_dict': policy_net.state_dict(),\n",
    "        'target_net_state_dict': target_net.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'avg_score': avg_score,\n",
    "        'timestamp': timestamp\n",
    "    }, filepath)\n",
    "    \n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(config, policy_net, target_net, optimizer, replay_buffer, device='cpu'):\n",
    "    \"\"\"\n",
    "    Train DQN with graceful interruption support and LR scheduling.\n",
    "    Press Ctrl+C anytime to stop and save progress.\n",
    "    \n",
    "    Supports:\n",
    "    - Standard DQN\n",
    "    - Double DQN\n",
    "    - Dueling DQN\n",
    "    - Prioritized Experience Replay (PER)\n",
    "    - Learning Rate Scheduling\n",
    "    \"\"\"\n",
    "    import psutil\n",
    "    from datetime import datetime\n",
    "    from torch.optim.lr_scheduler import ExponentialLR\n",
    "    import os\n",
    "    import numpy as np # Ensure numpy is available for printing\n",
    "    import torch.nn.functional as F # Ensure F is available for optimize functions\n",
    "    \n",
    "    # Print configuration\n",
    "    print_config(config)\n",
    "\n",
    "    # Create environment\n",
    "    env = gym.make(config['ENV_ID'])\n",
    "    if config.get('SEED') is not None:\n",
    "        env.reset(seed=config['SEED'])\n",
    "    \n",
    "    n_actions = config['N_ACTIONS']\n",
    "    episode_rewards = []\n",
    "    steps = 0\n",
    "    best_avg_score = -float('inf')\n",
    "    \n",
    "    # Initialize LR scheduler (optional - only if enabled in config)\n",
    "    scheduler = None\n",
    "    if config.get('LR_SCHEDULER', False):\n",
    "        scheduler = ExponentialLR(optimizer, gamma=config.get('LR_GAMMA', 0.9995))\n",
    "        print(f\"âœ… LR Scheduler enabled: ExponentialLR (gamma={config.get('LR_GAMMA', 0.9995)})\")\n",
    "    \n",
    "    # Select optimization function based on CONFIG\n",
    "    if config.get('USE_PER', False):\n",
    "        dqn_type = config.get('DQN_TYPE', 'DQN')\n",
    "        def optimize_fn():\n",
    "            return optimize_model_per(\n",
    "                policy_net, target_net, optimizer, replay_buffer,\n",
    "                config['BATCH_SIZE'], config['GAMMA'], device, \n",
    "                dqn_type\n",
    "            )\n",
    "        print(f\"Optimization: {dqn_type} + PER\")\n",
    "    elif config.get('DQN_TYPE', 'DQN') == 'DoubleDQN':\n",
    "        def optimize_fn():\n",
    "            return optimize_model_double_dqn(\n",
    "                policy_net, target_net, optimizer, replay_buffer,\n",
    "                config['BATCH_SIZE'], config['GAMMA'], device\n",
    "            )\n",
    "        print(\"Optimization: Double DQN\")\n",
    "    else:\n",
    "        def optimize_fn():\n",
    "            return optimize_model_dqn(\n",
    "                policy_net, target_net, optimizer, replay_buffer,\n",
    "                config['BATCH_SIZE'], config['GAMMA'], device\n",
    "            )\n",
    "        dqn_type = config.get('DQN_TYPE', 'DQN')\n",
    "        if dqn_type == 'DuelingDQN':\n",
    "            print(\"Optimization: Dueling DQN (uses Standard DQN optimization)\")\n",
    "        else:\n",
    "            print(\"Optimization: Standard DQN\")\n",
    "    \n",
    "    print(\"\\nðŸ’¡ Press Ctrl+C anytime to stop training and save progress\\n\")\n",
    "    \n",
    "    # --- START TIME LOGGING ---\n",
    "    start_time = datetime.now()\n",
    "    print(f\"--- TRAINING STARTED: {start_time.strftime('%Y-%m-%d %H:%M:%S')} ---\\n\")\n",
    "    # --------------------------\n",
    "    \n",
    "    try:\n",
    "        for episode in range(config['N_EPISODES']):\n",
    "            state, _ = env.reset()\n",
    "            # Ensure preprocess_frame is defined or imported globally\n",
    "            # For this context, we assume preprocess_frame is defined elsewhere\n",
    "            from collections import deque\n",
    "            state = preprocess_frame(state)\n",
    "            state_stack = deque([state] * config['N_FRAMES'], maxlen=config['N_FRAMES'])\n",
    "            \n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                epsilon = config['EPSILON_END'] + (config['EPSILON_START'] - config['EPSILON_END']) * \\\n",
    "                          np.exp(-1. * steps / config['EPSILON_DECAY'])\n",
    "                \n",
    "                state_array = np.array(state_stack)\n",
    "                # Ensure select_action is defined or imported globally\n",
    "                action = select_action(state_array, epsilon, policy_net, n_actions, device)\n",
    "                \n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                next_state = preprocess_frame(next_state)\n",
    "                next_state_stack = state_stack.copy()\n",
    "                next_state_stack.append(next_state)\n",
    "                \n",
    "                replay_buffer.push(\n",
    "                    np.array(state_stack),\n",
    "                    action,\n",
    "                    reward,\n",
    "                    np.array(next_state_stack),\n",
    "                    float(done)\n",
    "                )\n",
    "                \n",
    "                state_stack = next_state_stack\n",
    "                episode_reward += reward\n",
    "                steps += 1\n",
    "                \n",
    "                optimize_fn()\n",
    "                \n",
    "                if steps % config['TARGET_UPDATE'] == 0:\n",
    "                    target_net.load_state_dict(policy_net.state_dict())\n",
    "            \n",
    "            episode_rewards.append(episode_reward)\n",
    "            \n",
    "            # Step LR scheduler after each episode\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "            \n",
    "            # Print every 10 episodes the progress\n",
    "            if episode % 10 == 0:\n",
    "                avg_score = np.mean(episode_rewards[-100:]) if len(episode_rewards) >= 100 else np.mean(episode_rewards)\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "                print(f'Episode {episode} | Score: {episode_reward:.1f} | Avg: {avg_score:.2f} | Eps: {epsilon:.3f} | LR: {current_lr:.6f} | Steps: {steps}')\n",
    "            \n",
    "            if episode % config['CHECKPOINT_EVERY'] == 0:\n",
    "                # save_checkpoint(\n",
    "                #         config, policy_net, target_net, optimizer, \n",
    "                #         episode, avg_score, episode_rewards, is_best=False)\n",
    "                if avg_score > best_avg_score:\n",
    "                    best_avg_score = avg_score\n",
    "                    save_checkpoint(\n",
    "                        config, policy_net, target_net, optimizer, \n",
    "                        episode, avg_score, episode_rewards, is_best=True)\n",
    "                    \n",
    "            if episode % 40 == 0:\n",
    "                mem = psutil.virtual_memory()\n",
    "                gpu_mem_str = \"N/A\"\n",
    "                if torch.cuda.is_available():\n",
    "                    gpu_mem = torch.cuda.memory_allocated() / 1024**3\n",
    "                    gpu_mem_str = f\"{gpu_mem:.2f}GB\"\n",
    "                elif torch.backends.mps.is_available() and device.type == 'mps':\n",
    "                    gpu_mem_str = \"Active\"\n",
    "                print(f'\\t\\tRAM: {mem.percent:.1f}% | GPU: {gpu_mem_str} | Buffer: {len(replay_buffer)}/{config[\"BUFFER_SIZE\"]}')\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        end_time = datetime.now()\n",
    "        duration = end_time - start_time\n",
    "        \n",
    "        print(\"\\n\\n\" + \"=\"*70)\n",
    "        print(\"âš ï¸  TRAINING INTERRUPTED BY USER\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        if len(episode_rewards) > 0:\n",
    "            current_episode = len(episode_rewards) - 1\n",
    "            avg_score = np.mean(episode_rewards[-100:]) if len(episode_rewards) >= 100 else np.mean(episode_rewards)\n",
    "            \n",
    "            print(f\"\\nðŸ“Š Training Statistics at Interruption:\")\n",
    "            print(\"-\" * 70)\n",
    "            print(f\"  Episodes completed: {current_episode + 1} / {config['N_EPISODES']}\")\n",
    "            print(f\"  Total steps: {steps:,}\")\n",
    "            print(f\"  Last episode score: {episode_rewards[-1]:.1f}\")\n",
    "            print(f\"  Average score (last {min(100, len(episode_rewards))} episodes): {avg_score:.2f}\")\n",
    "            print(f\"  Best average score: {best_avg_score:.2f}\")\n",
    "            print(f\"  Max episode score: {max(episode_rewards):.1f}\")\n",
    "            print(f\"  Min episode score: {min(episode_rewards):.1f}\")\n",
    "            \n",
    "            # --- INTERRUPTED TIME LOGGING ---\n",
    "            print(f\"\\n--- INTERRUPTED AT: {end_time.strftime('%Y-%m-%d %H:%M:%S')} ---\")\n",
    "            print(f\"--- DURATION: {duration} ---\")\n",
    "            # --------------------------------\n",
    "            \n",
    "            save_checkpoint(\n",
    "                config, policy_net, target_net, optimizer, \n",
    "                current_episode, avg_score, episode_rewards, is_best=False)\n",
    "            \n",
    "            try:\n",
    "                import matplotlib.pyplot as plt\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                plt.plot(episode_rewards, alpha=0.6, label='Episode Reward')\n",
    "                if len(episode_rewards) >= 10:\n",
    "                    window = min(100, len(episode_rewards))\n",
    "                    moving_avg = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\n",
    "                    plt.plot(range(window-1, len(episode_rewards)), moving_avg, \n",
    "                            label=f'Moving Avg ({window})', linewidth=2, color='red')\n",
    "                plt.axhline(y=avg_score, color='green', linestyle='--', \n",
    "                           label=f'Current Avg: {avg_score:.2f}')\n",
    "                plt.xlabel('Episode')\n",
    "                plt.ylabel('Score')\n",
    "                plt.title(f'Training Progress (Interrupted at Episode {current_episode})')\n",
    "                plt.legend()\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                plt.tight_layout()\n",
    "                \n",
    "                plot_filename = f'interrupted_plot_{timestamp}.png'\n",
    "                plot_filepath = os.path.join(config['BEST_MODELS_DIR'], plot_filename)\n",
    "                plt.savefig(plot_filepath, dpi=100)\n",
    "                \n",
    "                print(f\"ðŸ“ˆ Plot saved: {plot_filepath}\")\n",
    "                plt.close()\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸  Could not generate plot: {e}\")\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"âœ… All progress saved successfully!\")\n",
    "            print(\"ðŸ’¡ You can resume training by loading the checkpoint\")\n",
    "            print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    finally:\n",
    "        env.close()\n",
    "    \n",
    "    if len(episode_rewards) == config['N_EPISODES']:\n",
    "        # --- END TIME LOGGING ---\n",
    "        end_time = datetime.now()\n",
    "        duration = end_time - start_time\n",
    "        print(f\"\\n--- TRAINING FINISHED: {end_time.strftime('%Y-%m-%d %H:%M:%S')} ---\")\n",
    "        print(f\"--- DURATION: {duration} ---\")\n",
    "        # ------------------------\n",
    "        \n",
    "        print(\"\\nâœ… Training completed successfully!\")\n",
    "        print(f\"Best average score: {best_avg_score:.2f}\\n\")\n",
    "    \n",
    "    return episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and Train - Standard DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Apple Silicon GPU)\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA GPU\")\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Apple Silicon GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU (will be slower)\")\n",
    "    \n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Configuration\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    'ENV_ID': 'ALE/SpaceInvaders-v5',\n",
    "    'SEED': 18,\n",
    "    'N_FRAMES': 4,\n",
    "    'N_ACTIONS': 6,\n",
    "    'N_EPISODES': 4000,\n",
    "    'LEARNING_RATE': 0.00001,\n",
    "    'GAMMA': 0.99,\n",
    "    'BATCH_SIZE': 32,\n",
    "    'EPSILON_START': 1.0,\n",
    "    'EPSILON_END': 0.1,\n",
    "    'EPSILON_DECAY': 100000,\n",
    "    'BUFFER_SIZE': 100000,\n",
    "    'TARGET_UPDATE': 5000,\n",
    "    'CHECKPOINT_EVERY': 400,\n",
    "    'USE_GDRIVE': USE_GDRIVE,\n",
    "    'CHECKPOINT_DIR': CHECKPOINT_DIR,\n",
    "    'BEST_MODELS_DIR': BEST_MODELS,\n",
    "    'DQN_TYPE': 'DQN',  # Options: 'DQN', 'DoubleDQN', 'DuelingDQN'\n",
    "    'USE_PER': False, \n",
    "    'LR_SCHEDULER': True,\n",
    "    'LR_GAMMA': 0.999, \n",
    "    \n",
    "    # PER hyperparameters (if USE_PER=True)\n",
    "    'PER_ALPHA': 0.6,\n",
    "    'PER_BETA_START': 0.4,\n",
    "    'PER_BETA_FRAMES': 100000,\n",
    "    'PER_EPSILON': 1e-6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Standard DQN\n",
    "CONFIG_DQN = BASE_CONFIG.copy()\n",
    "CONFIG_DQN['DQN_TYPE'] = 'DQN'\n",
    "CONFIG_DQN['USE_PER'] = False\n",
    "\n",
    "# Set random seeds\n",
    "random.seed(CONFIG_DQN['SEED'])\n",
    "np.random.seed(CONFIG_DQN['SEED'])\n",
    "torch.manual_seed(CONFIG_DQN['SEED'])\n",
    "\n",
    "# Networks\n",
    "policy_net_dqn = DQN((CONFIG_DQN['N_FRAMES'], 84, 84), CONFIG_DQN['N_ACTIONS']).to(device)\n",
    "target_net_dqn = DQN((CONFIG_DQN['N_FRAMES'], 84, 84), CONFIG_DQN['N_ACTIONS']).to(device)\n",
    "target_net_dqn.load_state_dict(policy_net_dqn.state_dict())\n",
    "\n",
    "optimizer_dqn = optim.Adam(policy_net_dqn.parameters(), lr=CONFIG_DQN['LEARNING_RATE'])\n",
    "replay_buffer_dqn = ReplayBuffer(CONFIG_DQN['BUFFER_SIZE'])\n",
    "\n",
    "# Train\n",
    "rewards_dqn = train_dqn(CONFIG_DQN, policy_net_dqn, target_net_dqn, \n",
    "                        optimizer_dqn, replay_buffer_dqn, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and Train - Double DQN (PER/no PER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Using Standard Uniform Replay Buffer\n",
      "\n",
      "======================================================================\n",
      "  DQN TYPE: DoubleDQN\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "----------------------------------------------------------------------\n",
      "  ENV_ID              : ALE/SpaceInvaders-v5\n",
      "  SEED                : 18\n",
      "  N_FRAMES            : 4\n",
      "  N_ACTIONS           : 6\n",
      "  N_EPISODES          : 4000\n",
      "  LEARNING_RATE       : 1e-05\n",
      "  GAMMA               : 0.99\n",
      "  BATCH_SIZE          : 64\n",
      "  EPSILON_START       : 1.0\n",
      "  EPSILON_END         : 0.1\n",
      "  EPSILON_DECAY       : 100000\n",
      "  BUFFER_SIZE         : 100000\n",
      "  TARGET_UPDATE       : 5000\n",
      "  CHECKPOINT_EVERY    : 400\n",
      "  USE_GDRIVE          : False\n",
      "  BEST_MODELS_DIR     : ./best_models\n",
      "  DQN_TYPE            : DoubleDQN\n",
      "  USE_PER             : False\n",
      "  LR_SCHEDULER        : True\n",
      "  LR_GAMMA            : 0.999\n",
      "  PER_ALPHA           : 0.6\n",
      "  PER_BETA_START      : 0.4\n",
      "  PER_BETA_FRAMES     : 120000\n",
      "  PER_EPSILON         : 1e-06\n",
      "======================================================================\n",
      "\n",
      "âœ… LR Scheduler enabled: ExponentialLR (gamma=0.999)\n",
      "Optimization: Double DQN\n",
      "\n",
      "ðŸ’¡ Press Ctrl+C anytime to stop training and save progress\n",
      "\n",
      "--- TRAINING STARTED: 2025-11-30 08:58:57 ---\n",
      "\n",
      "Episode 0 | Score: 55.0 | Avg: 55.00 | Eps: 0.996 | LR: 0.000010 | Steps: 409\n",
      "\t\tNew best avg: 55.00 - saved to DoubleDQN_20251130_085903_best.pth\n",
      "\t\tRAM: 67.5% | GPU: Active | Buffer: 409/100000\n",
      "Episode 10 | Score: 110.0 | Avg: 132.27 | Eps: 0.953 | LR: 0.000010 | Steps: 5393\n",
      "Episode 20 | Score: 235.0 | Avg: 125.71 | Eps: 0.912 | LR: 0.000010 | Steps: 10322\n",
      "Episode 30 | Score: 45.0 | Avg: 134.68 | Eps: 0.873 | LR: 0.000010 | Steps: 15256\n",
      "Episode 40 | Score: 180.0 | Avg: 141.10 | Eps: 0.826 | LR: 0.000010 | Steps: 21423\n",
      "\t\tRAM: 68.4% | GPU: Active | Buffer: 21423/100000\n"
     ]
    }
   ],
   "source": [
    "# Configuration for Double DQN\n",
    "CONFIG_DDQN = BASE_CONFIG.copy()\n",
    "CONFIG_DDQN['DQN_TYPE'] = 'DoubleDQN'\n",
    "CONFIG_DDQN['LEARNING_RATE'] = 0.00001\n",
    "# CONFIG_DDQN['LEARNING_RATE'] = 0.0005\n",
    "CONFIG_DDQN['TARGET_UPDATE'] = 5000\n",
    "CONFIG_DDQN['EPSILON_DECAY'] = 100000\n",
    "# CONFIG_DDQN['EPSILON_DECAY'] = 25000\n",
    "CONFIG_DDQN['BUFFER_SIZE'] = 100000\n",
    "# CONFIG_DDQN['BUFFER_SIZE'] = 25000\n",
    "CONFIG_DDQN['N_EPISODES'] = 4000\n",
    "CONFIG_DDQN['BATCH_SIZE'] = 64\n",
    "CONFIG_DDQN['PER_BETA_FRAMES'] = 120000\n",
    "CONFIG_DDQN['USE_PER'] = False\n",
    "\n",
    "# Set random seeds\n",
    "random.seed(CONFIG_DDQN['SEED'])\n",
    "np.random.seed(CONFIG_DDQN['SEED'])\n",
    "torch.manual_seed(CONFIG_DDQN['SEED'])\n",
    "\n",
    "# Networks\n",
    "policy_net_ddqn = DQN((CONFIG_DDQN['N_FRAMES'], 84, 84), CONFIG_DDQN['N_ACTIONS']).to(device)\n",
    "target_net_ddqn = DQN((CONFIG_DDQN['N_FRAMES'], 84, 84), CONFIG_DDQN['N_ACTIONS']).to(device)\n",
    "target_net_ddqn.load_state_dict(policy_net_ddqn.state_dict())\n",
    "\n",
    "optimizer_ddqn = optim.Adam(policy_net_ddqn.parameters(), lr=CONFIG_DDQN['LEARNING_RATE'])\n",
    "\n",
    "if CONFIG_DDQN['USE_PER']:\n",
    "    replay_buffer = PrioritizedReplayBuffer(\n",
    "        CONFIG_DDQN.get('BUFFER_SIZE', 10000),\n",
    "        alpha=CONFIG_DDQN.get('PER_ALPHA', 0.6),\n",
    "        beta_start=CONFIG_DDQN.get('PER_BETA_START', 0.4),\n",
    "        beta_frames=CONFIG_DDQN.get('PER_BETA_FRAMES', 100000)\n",
    "    )\n",
    "    print(\"   âœ… Using Prioritized Experience Replay (PER)\")\n",
    "    print(f\"      Alpha: {CONFIG_DDQN.get('PER_ALPHA', 0.6)}\")\n",
    "    print(f\"      Beta: {CONFIG_DDQN.get('PER_BETA_START', 0.4)} â†’ 1.0\")\n",
    "else:\n",
    "    replay_buffer = ReplayBuffer(CONFIG_DDQN.get('BUFFER_SIZE', 10000))\n",
    "    print(\"   âœ… Using Standard Uniform Replay Buffer\")\n",
    "\n",
    "# Train\n",
    "rewards_ddqn = train_dqn(CONFIG_DDQN, policy_net_ddqn, target_net_ddqn, \n",
    "                         optimizer_ddqn, replay_buffer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and Train - Dueling DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Dueling DQN\n",
    "CONFIG_DuelDQN = BASE_CONFIG.copy()\n",
    "CONFIG_DuelDQN['DQN_TYPE'] = 'DuelingDQN'\n",
    "CONFIG_DuelDQN['USE_PER'] = False\n",
    "\n",
    "# Set random seeds\n",
    "random.seed(CONFIG_DuelDQN['SEED'])\n",
    "np.random.seed(CONFIG_DuelDQN['SEED'])\n",
    "torch.manual_seed(CONFIG_DuelDQN['SEED'])\n",
    "\n",
    "# Networks - Use DuelingDQN architecture\n",
    "policy_net_dueling = DuelingDQN((CONFIG_DuelDQN['N_FRAMES'], 84, 84), CONFIG_DuelDQN['N_ACTIONS']).to(device)\n",
    "target_net_dueling = DuelingDQN((CONFIG_DuelDQN['N_FRAMES'], 84, 84), CONFIG_DuelDQN['N_ACTIONS']).to(device)\n",
    "target_net_dueling.load_state_dict(policy_net_dueling.state_dict())\n",
    "\n",
    "optimizer_dueling = optim.Adam(policy_net_dueling.parameters(), lr=CONFIG_DuelDQN['LEARNING_RATE'])\n",
    "replay_buffer_dueling = ReplayBuffer(CONFIG_DuelDQN['BUFFER_SIZE'])\n",
    "\n",
    "# Train\n",
    "rewards_dueling = train_dqn(CONFIG_DuelDQN, policy_net_dueling, target_net_dueling, \n",
    "                            optimizer_dueling, replay_buffer_dueling, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and Train - DQN with PER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for DQN with Prioritized Experience Replay\n",
    "CONFIG_PER = BASE_CONFIG.copy()\n",
    "CONFIG_PER['DQN_TYPE'] = 'DQN'\n",
    "CONFIG_PER['USE_PER'] = True\n",
    "\n",
    "CONFIG_PER['LEARNING_RATE'] = 0.00035\n",
    "CONFIG_PER['TARGET_UPDATE'] = 2000\n",
    "CONFIG_PER['EPSILON_DECAY'] = 20000\n",
    "\n",
    "CONFIG_PER['BUFFER_SIZE'] = 20000\n",
    "\n",
    "# Set random seeds\n",
    "random.seed(CONFIG_PER['SEED'])\n",
    "np.random.seed(CONFIG_PER['SEED'])\n",
    "torch.manual_seed(CONFIG_PER['SEED'])\n",
    "\n",
    "# Networks\n",
    "policy_net_per = DQN((CONFIG_PER['N_FRAMES'], 84, 84), CONFIG_PER['N_ACTIONS']).to(device)\n",
    "target_net_per = DQN((CONFIG_PER['N_FRAMES'], 84, 84), CONFIG_PER['N_ACTIONS']).to(device)\n",
    "target_net_per.load_state_dict(policy_net_per.state_dict())\n",
    "\n",
    "optimizer_per = optim.Adam(policy_net_per.parameters(), lr=CONFIG_PER['LEARNING_RATE'])\n",
    "replay_buffer_per = PrioritizedReplayBuffer(\n",
    "    CONFIG_PER['BUFFER_SIZE'],\n",
    "    alpha=CONFIG_PER['PER_ALPHA'],\n",
    "    beta_start=CONFIG_PER['PER_BETA_START'],\n",
    "    beta_frames=CONFIG_PER['PER_BETA_FRAMES']\n",
    ")\n",
    "\n",
    "# Train\n",
    "rewards_per = train_dqn(CONFIG_PER, policy_net_per, target_net_per, \n",
    "                        optimizer_per, replay_buffer_per, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidated Results Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all results\n",
    "all_results = {\n",
    "    'DQN': rewards_dqn,\n",
    "    'DoubleDQN': rewards_ddqn,\n",
    "    'DuelingDQN': rewards_dueling,\n",
    "    'DQN_PER': rewards_per\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_consolidated_results(results_dict, window=100, figsize=(14, 8)):\n",
    "    \"\"\"\n",
    "    Plot consolidated training progress for multiple DQN runs.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
    "    \n",
    "    for idx, (name, rewards) in enumerate(results_dict.items()):\n",
    "        # Calculate moving average\n",
    "        if len(rewards) >= window:\n",
    "            moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "            episodes = range(window-1, len(rewards))\n",
    "            final_avg = np.mean(rewards[-100:])\n",
    "            \n",
    "            # Plot moving average\n",
    "            plt.plot(episodes, moving_avg, \n",
    "                    label=f'{name} (Avg={final_avg:.2f})',\n",
    "                    color=colors[idx % len(colors)],\n",
    "                    linewidth=2)\n",
    "    \n",
    "    # Add goal lines\n",
    "    plt.axhline(y=500, color='green', linestyle='--', linewidth=2, label='Goal: 500', alpha=0.7)\n",
    "    plt.axhline(y=400, color='red', linestyle='--', linewidth=2, label='Goal: 400', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Episode #', fontsize=12)\n",
    "    plt.ylabel(f'Average Score ({window}-Game Window)', fontsize=12)\n",
    "    plt.title(f'Consolidated DQN Training Progress ({window}-Episode Moving Average)', fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc='best', fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_individual_results(rewards, name, window=100, figsize=(12, 6)):\n",
    "    \"\"\"Plot individual run results\"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(rewards, alpha=0.6, label='Episode Reward')\n",
    "    \n",
    "    # Calculate moving average\n",
    "    moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    plt.plot(range(window-1, len(rewards)), moving_avg, label=f'Moving Average ({window})', linewidth=2)\n",
    "    \n",
    "    plt.axhline(y=500, color='r', linestyle='--', label='Target (500)')\n",
    "    plt.axhline(y=400, color='orange', linestyle='--', label='Minimum (400)')\n",
    "    \n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title(f'{name} - Training Progress on Space Invaders')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print final statistics\n",
    "    final_avg = np.mean(rewards[-100:])\n",
    "    print(f\"\\n{name} - Final average reward (last 100 episodes): {final_avg:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot consolidated results\n",
    "plot_consolidated_results(all_results, window=100)\n",
    "\n",
    "# Plot individual results\n",
    "for name, rewards in all_results.items():\n",
    "    plot_individual_results(rewards, name, window=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results to a single file\n",
    "import pickle\n",
    "\n",
    "results_file = os.path.join(CHECKPOINT_DIR, 'all_results.pkl')\n",
    "with open(results_file, 'wb') as f:\n",
    "    pickle.dump(all_results, f)\n",
    "\n",
    "print(f\"All results saved to: {results_file}\")\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "for name, rewards in all_results.items():\n",
    "    final_avg = np.mean(rewards[-100:])\n",
    "    max_reward = max(rewards)\n",
    "    print(f\"{name:20s} - Avg (last 100): {final_avg:6.2f} | Max: {max_reward:6.1f}\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_msc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
