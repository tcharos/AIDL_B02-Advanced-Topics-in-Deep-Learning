{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tcharos/AIDL_B02-Advanced-Topics-in-Deep-Learning/blob/main/AIDL_B02_AdvancedTopicsInDeepLearning_SpaceInvaders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkZJMnJp0uUq"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tcharos/AIDL_B02-Advanced-Topics-in-Deep-Learning/blob/main/AIDL_B02_AdvancedTopicsInDeepLearning_SpaceInvaders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "# ðŸš€ Project Base: DQN Variants for ALE/SpaceInvaders-v5\n",
        "\n",
        "This notebook strictly implements the project's requirements for the **`ALE/SpaceInvaders-v5`** environment with 4-frame stacking and CNN architecture.\n",
        "\n",
        "**Key Requirements Met:**\n",
        "* **Environment:** `ALE/SpaceInvaders-v5` [cite: 11]\n",
        "* **Action Space:** 6 actions [cite: 13, 21]\n",
        "* **State:** 4 stacked input frames [cite: 19]\n",
        "\n",
        "**To run an implementation:**\n",
        "1.  Change the `CONFIG['MODE']` variable below to one of: **`SimpleDQN`**, **`DoubleDQN`**, or **`DuelingDQN`**.\n",
        "2.  Adjust hyperparameters (`LR`, `EPS_DECAY`, etc.) in the `CONFIG` dictionary if needed.\n",
        "3.  Run all cells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## 1. Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "!pip install \"gymnasium[atari,accept-rom-license,other]\" ale-py\n",
        "!pip install pyvirtualdisplay\n",
        "!apt-get install -y xvfb x11-utils\n",
        "!pip install shimmy imageio-ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "config"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from collections import deque, namedtuple\n",
        "import matplotlib.pyplot as plt\n",
        "from gymnasium.wrappers import AtariPreprocessing\n",
        "from gymnasium.wrappers import FrameStackObservation\n",
        "import ale_py\n",
        "\n",
        "import gc\n",
        "import psutil\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Tools for video display\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "from base64 import b64encode"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- GLOBAL CONFIGURATION -----------------\n",
        "CONFIG = {\n",
        "    \"ENV_ID\": 'ALE/SpaceInvaders-v5',\n",
        "    \"SEED\": 2,\n",
        "    \"MODE\": \"SimpleDQN\", # 'SimpleDQN', 'DoubleDQN', 'DuelingDQN'\n",
        "    \"INPUT_SHAPE\": (4, 84, 84), # 4 stacked frames, resized to 84x84\n",
        "    \"BUFFER_SIZE\": int(5e4), # also checked 1e5, 1e4 (10000), 5e3 (5000), 7.5e3 (7500)\n",
        "    \"BATCH_SIZE\": 64, # Reduced batch size\n",
        "    \"GAMMA\": 0.99, # Prioritize long-term cumulative reward\n",
        "    \"TAU\": 1e-3, # Soft Update Rate\n",
        "    \"LR\": 2e-4, # Also tried 1e-4, 5e-5. Lower learning rate --> stable convergence\n",
        "    \"UPDATE_EVERY\": 4, # Learn frequency\n",
        "    \"TARGET_UPDATE_FREQ\": 1000, # Also tried 1000\n",
        "    \"N_EPISODES\": 5000,\n",
        "    \"EPS_START\": 1.0, # Initial probability of choosing a random action (exploration) --> fully exploring the environment to gather initial experiences\n",
        "    \"EPS_END\": 0.01, # Also chekced 0.01. Minimum probability of choosing a random action.\n",
        "    \"EPS_DECAY\": 0.9996, # Also checked 0.999, 0.995\n",
        "    \"USE_GOOGLE_DRIVE\": True,\n",
        "    \"CHECKPOINT_FREQ\": 400,\n",
        "    \"GOAL_SCORE\": 400.0\n",
        "}\n",
        "# --------------------------------------------------------\n",
        "\n",
        "gym.register_envs(ale_py)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "random.seed(CONFIG['SEED'])\n",
        "np.random.seed(CONFIG['SEED'])\n",
        "torch.manual_seed(CONFIG['SEED'])\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(CONFIG['SEED'])\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"Current DQN Mode: {CONFIG['MODE']}\")"
      ],
      "metadata": {
        "id": "rSTjoTix3Zdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print configuration\n",
        "def print_config():\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"CURRENT CONFIGURATION - SimpleDQN Aggressive\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Environment:          {CONFIG['ENV_ID']}\")\n",
        "    print(f\"Seed:                 {CONFIG['SEED']}\")\n",
        "    print(f\"Mode:                 {CONFIG['MODE']}\")\n",
        "    print(f\"Buffer Size:          {CONFIG['BUFFER_SIZE']:,} experiences\")\n",
        "    print(f\"Batch Size:           {CONFIG['BATCH_SIZE']}\")\n",
        "    print(f\"Learning Rate:        {CONFIG['LR']}\")\n",
        "    print(f\"Gamma:                {CONFIG['GAMMA']}\")\n",
        "    print(f\"Update Every:         {CONFIG['UPDATE_EVERY']} steps\")\n",
        "    print(f\"Target Update Freq:   {CONFIG['TARGET_UPDATE_FREQ']} steps\")\n",
        "    print(f\"Episodes:             {CONFIG['N_EPISODES']}\")\n",
        "    print(f\"Epsilon Start:        {CONFIG['EPS_START']}\")\n",
        "    print(f\"Epsilon End:          {CONFIG['EPS_END']}\")\n",
        "    print(f\"Epsilon Decay:        {CONFIG['EPS_DECAY']}\")\n",
        "    print(f\"Checkpoint Freq:      {CONFIG['CHECKPOINT_FREQ']} episodes\")\n",
        "    print(\"=\"*70)"
      ],
      "metadata": {
        "id": "f6mh-9y1ix7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if CONFIG['USE_GOOGLE_DRIVE']:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    checkpoint_dir = '/content/drive/MyDrive/DQN_Checkpoints'\n",
        "    print(\"âœ“ Google Drive mounted - checkpoints will be saved to Drive\")\n",
        "else:\n",
        "    checkpoint_dir = './checkpoints'  # Local directory\n",
        "    print(\"âœ“ Using local storage - checkpoints will be saved locally\")\n",
        "\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "7_nOa0elCNZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "env_init"
      },
      "source": [
        "## 2. Environment Initialization\n",
        "We use **`AtariPreprocessing`** to handle resizing/cropping to 84x84 and grayscale conversion. **`FrameStack`** then stacks 4 consecutive frames, fulfilling the requirements for the state space[cite: 19, 20]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "env_run"
      },
      "outputs": [],
      "source": [
        "def make_atari_env(env_id, seed):\n",
        "    \"\"\"Creates and wraps the Atari environment with standard preprocessing and 4-frame stacking.\"\"\"\n",
        "    # 1. Base Environment (Using the required ID [cite: 11])\n",
        "    env = gym.make(env_id)\n",
        "\n",
        "    # 2. Atari Preprocessing: Resizes to 84x84, grayscale, handles max-pooling/skip.\n",
        "    # Frame skip is set to 1 here because the ALE/SpaceInvaders-v5 environment generally handles skips\n",
        "    # implicitly, or we rely on the standard wrappers' internal logic for compatibility.\n",
        "    env = AtariPreprocessing(env, grayscale_obs=True, terminal_on_life_loss=True, frame_skip=1, screen_size=84)\n",
        "\n",
        "    # 3. Frame Stacking (Creates the (4, 84, 84) state [cite: 19])\n",
        "    env = FrameStackObservation(env, stack_size=4)\n",
        "\n",
        "    # Set seed on the final environment\n",
        "    if seed is not None:\n",
        "        env.action_space.seed(seed)\n",
        "        env.observation_space.seed(seed)\n",
        "\n",
        "    return env\n",
        "\n",
        "env = make_atari_env(CONFIG['ENV_ID'], CONFIG['SEED'])\n",
        "action_size = env.action_space.n\n",
        "state_shape = env.observation_space.shape\n",
        "\n",
        "print(f'Final State shape (Stacked Frames): {state_shape}')\n",
        "print(f'Number of available actions (SpaceInvaders): {action_size}') # Confirms 6 actions [cite: 13, 21]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "network_arch"
      },
      "source": [
        "## 3. Q-Network Architecture\n",
        "The network uses a CNN architecture  to process the high-dimensional image input, supporting Dueling components via a flag."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNetwork_code"
      },
      "outputs": [],
      "source": [
        "class QNetwork(nn.Module):\n",
        "    \"\"\"Enhanced Q-Network for better performance.\"\"\"\n",
        "\n",
        "    def __init__(self, state_shape, action_size, seed, dueling=False):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.dueling = dueling\n",
        "        in_channels = state_shape[0]\n",
        "\n",
        "        # Enhanced CNN layers with BatchNorm\n",
        "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1)  # Increased to 128\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "\n",
        "        # Calculate fc input size\n",
        "        dummy_input = torch.zeros(1, *state_shape)\n",
        "        x = self._forward_conv(dummy_input)\n",
        "        self.fc_input_size = x.view(1, -1).size(1)\n",
        "\n",
        "        if self.dueling:\n",
        "            # Dueling streams\n",
        "            self.fc_v1 = nn.Linear(self.fc_input_size, 512)\n",
        "            self.fc_v2 = nn.Linear(512, 256)  # Extra layer\n",
        "            self.fc_v3 = nn.Linear(256, 1)\n",
        "\n",
        "            self.fc_a1 = nn.Linear(self.fc_input_size, 512)\n",
        "            self.fc_a2 = nn.Linear(512, 256)  # Extra layer\n",
        "            self.fc_a3 = nn.Linear(256, action_size)\n",
        "\n",
        "            self.dropout = nn.Dropout(0.15)\n",
        "        else:\n",
        "            # Standard stream with extra layer\n",
        "            self.fc1 = nn.Linear(self.fc_input_size, 512)\n",
        "            self.fc2 = nn.Linear(512, 256)  # Extra layer\n",
        "            self.fc3 = nn.Linear(256, action_size)\n",
        "\n",
        "            self.dropout = nn.Dropout(0.15)\n",
        "\n",
        "    def _forward_conv(self, x):\n",
        "        \"\"\"Forward through conv with BatchNorm.\"\"\"\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "    def forward(self, state):\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "        x = self._forward_conv(state)\n",
        "\n",
        "        if self.dueling:\n",
        "            v = F.relu(self.fc_v1(x))\n",
        "            v = F.relu(self.fc_v2(v))\n",
        "            v = self.dropout(v)\n",
        "            v = self.fc_v3(v)\n",
        "\n",
        "            a = F.relu(self.fc_a1(x))\n",
        "            a = F.relu(self.fc_a2(a))\n",
        "            a = self.dropout(a)\n",
        "            a = self.fc_a3(a)\n",
        "\n",
        "            return v + a - a.mean(1, keepdim=True)\n",
        "        else:\n",
        "            x = F.relu(self.fc1(x))\n",
        "            x = F.relu(self.fc2(x))\n",
        "            x = self.dropout(x)\n",
        "            return self.fc3(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print architecture\n",
        "def print_network_architecture(network):\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"NEURAL NETWORK ARCHITECTURE\")\n",
        "    print(\"=\"*70)\n",
        "    print(network)\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in network.parameters())\n",
        "    trainable_params = sum(p.numel() for p in network.parameters() if p.requires_grad)\n",
        "\n",
        "    print(f\"\\nTotal Parameters: {total_params:,}\")\n",
        "    print(f\"Trainable Parameters: {trainable_params:,}\")\n",
        "    print(\"=\"*70 + \"\\n\")"
      ],
      "metadata": {
        "id": "GCz3WNEfnwwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buffer_agents"
      },
      "source": [
        "## 4. Replay Buffer and Agent Implementations\n",
        "The **Replay Buffer** (PER is an optional extension [cite: 27]) is crucial for breaking correlation in experience samples. The **AgentBase** handles common functions; specialized classes implement the specific Q-learning update rule."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReplayBuffer_code"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"Memory-efficient replay buffer using uint8 for frames.\"\"\"\n",
        "\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Store frames as uint8 to save memory (4x compression).\"\"\"\n",
        "        # Convert float32 [0,1] to uint8 [0,255]\n",
        "        if state.dtype == np.float32 or state.dtype == np.float64:\n",
        "            state = (state * 255).astype(np.uint8)\n",
        "        if next_state.dtype == np.float32 or next_state.dtype == np.float64:\n",
        "            next_state = (next_state * 255).astype(np.uint8)\n",
        "\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"Sample and convert back to float32 [0,1].\"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(np.stack([e.state for e in experiences])).float().to(device) / 255.0\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences])).long().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.stack([e.next_state for e in experiences])).float().to(device) / 255.0\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences]).astype(np.uint8)).float().to(device)\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Agent_init"
      },
      "outputs": [],
      "source": [
        "class AgentBase:\n",
        "    \"\"\"Base class for all DQN agents, handling shared components and target network logic.\"\"\"\n",
        "\n",
        "    def __init__(self, state_shape, action_size, seed, mode, dueling):\n",
        "        self.state_shape = state_shape\n",
        "        self.action_size = action_size\n",
        "        self.mode = mode\n",
        "\n",
        "        # Initialize Q-Networks\n",
        "        self.qnetwork_local = QNetwork(state_shape, action_size, seed, dueling=dueling).to(device)\n",
        "        self.qnetwork_target = QNetwork(state_shape, action_size, seed, dueling=dueling).to(device)\n",
        "        self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())\n",
        "\n",
        "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=CONFIG['LR'])\n",
        "        self.memory = ReplayBuffer(action_size, CONFIG['BUFFER_SIZE'], CONFIG['BATCH_SIZE'], seed)\n",
        "\n",
        "        self.t_step = 0\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "      # Convert LazyFrames to numpy if needed\n",
        "      if hasattr(state, '__array__'):\n",
        "          state = np.array(state)\n",
        "      if hasattr(next_state, '__array__'):\n",
        "          next_state = np.array(next_state)\n",
        "\n",
        "      # Save experience\n",
        "      self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "      # Learn every UPDATE_EVERY steps\n",
        "      self.t_step = (self.t_step + 1) % CONFIG['UPDATE_EVERY']\n",
        "      if self.t_step == 0:\n",
        "          if len(self.memory) > CONFIG['BATCH_SIZE']:\n",
        "              experiences = self.memory.sample()\n",
        "              self.learn(experiences, CONFIG['GAMMA'])\n",
        "              # CRITICAL: Delete experiences tuple to free memory\n",
        "              del experiences\n",
        "\n",
        "      # Hard update the target network periodically\n",
        "      # FIX: This condition was wrong - it would almost never trigger\n",
        "      if (self.t_step + 1) % CONFIG['TARGET_UPDATE_FREQ'] == 0:\n",
        "          self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "      \"\"\"Returns action based on epsilon-greedy policy.\"\"\"\n",
        "      # Convert LazyFrames to numpy if needed\n",
        "      if hasattr(state, '__array__'):\n",
        "          state = np.array(state)\n",
        "\n",
        "      state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "\n",
        "      self.qnetwork_local.eval()\n",
        "      with torch.no_grad():\n",
        "          action_values = self.qnetwork_local(state)\n",
        "      self.qnetwork_local.train()\n",
        "\n",
        "      if random.random() > eps:\n",
        "          return np.argmax(action_values.cpu().data.numpy())\n",
        "      else:\n",
        "          return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        # Placeholder, implemented by child classes\n",
        "        pass\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# --- ðŸ’¥ DQN Variant 1: Simple DQN (Original Target Calculation) ---\n",
        "# ----------------------------------------------------------------\n",
        "class SimpleDQNAgent(AgentBase):\n",
        "    \"\"\"Implements the original DQN learning step: Target Q = R + gamma * max_a Q_target(s', a).\"\"\"\n",
        "    def __init__(self, state_shape, action_size, seed):\n",
        "        # Initialize with Standard QNetwork (dueling=False)\n",
        "        super().__init__(state_shape, action_size, seed, mode='SimpleDQN', dueling=False)\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        # Ensure states have the right shape: (batch, 4, 84, 84)\n",
        "        if states.dim() == 5:  # If shape is (batch, 4, 1, 84, 84)\n",
        "            states = states.squeeze(2)\n",
        "        if next_states.dim() == 5:\n",
        "            next_states = next_states.squeeze(2)\n",
        "\n",
        "        # Target Q calculation uses the max Q-value from the target network directly.\n",
        "        with torch.no_grad():  # â† CRITICAL: Prevent gradient tracking for target\n",
        "            Q_targets_next = self.qnetwork_target(next_states).max(1)[0].unsqueeze(1)\n",
        "            Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "\n",
        "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "\n",
        "        # loss = F.mse_loss(Q_expected, Q_targets)\n",
        "        loss = F.smooth_l1_loss(Q_expected, Q_targets)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.qnetwork_local.parameters(), 10.0)\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # CRITICAL: Clean up to prevent memory leak\n",
        "        del states, actions, rewards, next_states, dones, Q_targets, Q_expected, loss\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# --- ðŸ’¥ DQN Variant 2: Double DQN (Decoupled Target Calculation) ---\n",
        "# ----------------------------------------------------------------\n",
        "class DoubleDQNAgent(AgentBase):\n",
        "    \"\"\"Implements the Double DQN learning step.\"\"\"\n",
        "    def __init__(self, state_shape, action_size, seed):\n",
        "        super().__init__(state_shape, action_size, seed, mode='DoubleDQN', dueling=False)\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        # Ensure states have the right shape\n",
        "        if states.dim() == 5:\n",
        "            states = states.squeeze(2)\n",
        "        if next_states.dim() == 5:\n",
        "            next_states = next_states.squeeze(2)\n",
        "\n",
        "        # Double DQN: Select actions with local, evaluate with target\n",
        "        with torch.no_grad():  # â† CRITICAL: No gradient tracking\n",
        "            # 1. Action selection from LOCAL network\n",
        "            Q_local_next = self.qnetwork_local(next_states)\n",
        "            best_actions = Q_local_next.max(1)[1].unsqueeze(1)\n",
        "\n",
        "            # 2. Value estimation from TARGET network\n",
        "            Q_targets_next = self.qnetwork_target(next_states).gather(1, best_actions)\n",
        "            Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "\n",
        "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "\n",
        "        loss = F.mse_loss(Q_expected, Q_targets)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Clean up\n",
        "        del states, actions, rewards, next_states, dones, Q_targets, Q_expected, loss\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# --- ðŸ’¥ DQN Variant 3: Dueling DQN (Dueling Architecture + Double Learning Rule) ---\n",
        "# ----------------------------------------------------------------\n",
        "class DuelingDQNAgent(DoubleDQNAgent):\n",
        "    \"\"\"Dueling DQN uses the Dueling architecture and the Double DQN learning rule for stability.\"\"\"\n",
        "    def __init__(self, state_shape, action_size, seed):\n",
        "        # Initialize with Dueling QNetwork (dueling=True)\n",
        "        AgentBase.__init__(self, state_shape, action_size, seed, mode='DuelingDQN', dueling=True)\n",
        "\n",
        "    # Inherits the Double DQN learn() method for stability\n",
        "\n",
        "\n",
        "# --- Agent Initialization based on global CONFIG['MODE'] ---\n",
        "if CONFIG['MODE'] == \"SimpleDQN\":\n",
        "    agent = SimpleDQNAgent(state_shape=state_shape, action_size=action_size, seed=CONFIG['SEED'])\n",
        "elif CONFIG['MODE'] == \"DoubleDQN\":\n",
        "    agent = DoubleDQNAgent(state_shape=state_shape, action_size=action_size, seed=CONFIG['SEED'])\n",
        "elif CONFIG['MODE'] == \"DuelingDQN\":\n",
        "    agent = DuelingDQNAgent(state_shape=state_shape, action_size=action_size, seed=CONFIG['SEED'])\n",
        "else:\n",
        "    raise ValueError(\"Invalid MODE specified in CONFIG.\")\n",
        "\n",
        "print(f\"Initialized agent: {type(agent).__name__} with learning mode: {agent.mode}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training_loop"
      },
      "source": [
        "## 5. Training and Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training_code"
      },
      "outputs": [],
      "source": [
        "def dqn_train(n_episodes=CONFIG['N_EPISODES'], max_t=10000, eps_start=CONFIG['EPS_START'], eps_end=CONFIG['EPS_END'], eps_decay=CONFIG['EPS_DECAY'], checkpoint_dir='/content/drive/MyDrive/DQN_Checkpoints'):\n",
        "    \"\"\"Deep Q-Learning with crash debugging.\"\"\"\n",
        "\n",
        "    import traceback\n",
        "\n",
        "    scores = []\n",
        "    scores_window = deque(maxlen=100)\n",
        "    eps = eps_start\n",
        "\n",
        "    best_avg_score = -float('inf')\n",
        "\n",
        "    global ALL_SCORES\n",
        "\n",
        "    print_config()\n",
        "\n",
        "    print(f\"\\nStarting training for {agent.mode}...\")\n",
        "    print(f\"LR: {CONFIG['LR']}, Buffer: {CONFIG['BUFFER_SIZE']}, Target Update: {CONFIG['TARGET_UPDATE_FREQ']}\")\n",
        "    print(f\"Checkpoints will be saved to: {checkpoint_dir}\")\n",
        "\n",
        "    try:\n",
        "        for i_episode in range(1, n_episodes + 1):\n",
        "            try:\n",
        "                # Monitor memory at start of episode\n",
        "                if i_episode % 10 == 0:\n",
        "                    mem = psutil.virtual_memory()\n",
        "                    gpu_mem = torch.cuda.memory_allocated() / (1024**3) if torch.cuda.is_available() else 0\n",
        "                    print(f'\\n[Ep {i_episode}] RAM: {mem.percent:.1f}% | GPU: {gpu_mem:.2f}GB | Buffer: {len(agent.memory)}/{CONFIG[\"BUFFER_SIZE\"]}')\n",
        "\n",
        "                state, info = env.reset(seed=CONFIG['SEED'] if i_episode == 1 else None)\n",
        "\n",
        "                # Check state validity\n",
        "                if state is None:\n",
        "                    print(f\"ERROR: Episode {i_episode} - env.reset() returned None!\")\n",
        "                    continue\n",
        "\n",
        "                state = np.array(state)\n",
        "\n",
        "                # Verify state shape\n",
        "                if state.shape != (4, 84, 84):\n",
        "                    print(f\"ERROR: Episode {i_episode} - Invalid state shape: {state.shape}\")\n",
        "                    continue\n",
        "\n",
        "                score = 0\n",
        "\n",
        "                for t in range(max_t):\n",
        "                    action = agent.act(state, eps)\n",
        "\n",
        "                    next_state_raw, reward, terminated, truncated, info = env.step(action)\n",
        "                    done = terminated or truncated\n",
        "\n",
        "                    # Check for NaN or invalid values\n",
        "                    # if np.isnan(reward) or np.isinf(reward):\n",
        "                    #     print(f\"WARNING: Episode {i_episode}, step {t} - Invalid reward: {reward}\")\n",
        "                    #     reward = 0.0\n",
        "\n",
        "                    next_state = np.array(next_state_raw)\n",
        "\n",
        "                    # Verify next_state\n",
        "                    # if next_state.shape != (4, 84, 84):\n",
        "                    #     print(f\"ERROR: Episode {i_episode}, step {t} - Invalid next_state shape: {next_state.shape}\")\n",
        "                    #     break\n",
        "\n",
        "                    reward_np = np.array([reward]).astype(np.float32)\n",
        "                    done_np = np.array([done]).astype(np.uint8)\n",
        "\n",
        "                    agent.step(state, action, reward_np, next_state, done_np)\n",
        "                    state = next_state\n",
        "                    score += reward\n",
        "\n",
        "                    if done:\n",
        "                        break\n",
        "\n",
        "                scores_window.append(score)\n",
        "                scores.append(score)\n",
        "                eps = max(eps_end, eps_decay * eps)\n",
        "                avg_score = np.mean(scores_window)\n",
        "\n",
        "                # Adaptive Learning Rate: Reduce LR if performance plateaus\n",
        "                if i_episode == 600:\n",
        "                  for param_group in agent.optimizer.param_groups:\n",
        "                      param_group['lr'] = 2.5e-5  # Half again\n",
        "                  print(f\"\\nâ†’ Reduced LR to 2.5e-5 at episode {i_episode}\")\n",
        "\n",
        "                if i_episode == 1200:\n",
        "                    for param_group in agent.optimizer.param_groups:\n",
        "                        param_group['lr'] = 1e-5  # Very fine tuning\n",
        "                    print(f\"\\nâ†’ Reduced LR to 1e-5 at episode {i_episode}\")\n",
        "\n",
        "                # Print progress\n",
        "                if i_episode % 1 == 0:  # Print every episode for debugging\n",
        "                    print(f'\\rEpisode {i_episode}\\tScore: {score:.1f}\\tAvg: {avg_score:.2f}\\tEps: {eps:.3f}\\tSteps: {t+1}', end=\"\")\n",
        "\n",
        "                # Frequent checkpoints while debugging\n",
        "                if i_episode % CONFIG['CHECKPOINT_FREQ'] == 0:\n",
        "                    print(f'\\n[CHECKPOINT] Episode {i_episode}\\tAverage Score: {avg_score:.2f}')\n",
        "\n",
        "                    checkpoint = {\n",
        "                        'episode': i_episode,\n",
        "                        'model_state_dict': agent.qnetwork_local.state_dict(),\n",
        "                        'target_state_dict': agent.qnetwork_target.state_dict(),\n",
        "                        'optimizer_state_dict': agent.optimizer.state_dict(),\n",
        "                        'scores': scores,\n",
        "                        'eps': eps,\n",
        "                        'mode': agent.mode\n",
        "                    }\n",
        "                    checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_{agent.mode}_ep{i_episode}.pth')\n",
        "                    torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "                    # Save best model separately\n",
        "                    if avg_score > best_avg_score:\n",
        "                        best_avg_score = avg_score\n",
        "                        best_path = os.path.join(checkpoint_dir, f'{agent.mode}_BEST.pth')\n",
        "                        torch.save(checkpoint, best_path)\n",
        "                        print(f'âœ“ New best avg: {avg_score:.2f} - saved to {agent.mode}_BEST.pth')\n",
        "                    else:\n",
        "                        print(f'âœ“ Checkpoint saved')\n",
        "\n",
        "                    # Aggressive memory cleanup\n",
        "                    gc.collect()\n",
        "                    if torch.cuda.is_available():\n",
        "                        torch.cuda.empty_cache()\n",
        "\n",
        "                    mem = psutil.virtual_memory()\n",
        "                    print(f\"RAM: {mem.percent}% ({mem.available / (1024**3):.1f}GB free)\")\n",
        "\n",
        "                # Check for goal\n",
        "                if avg_score >= CONFIG['GOAL_SCORE']:\n",
        "                    print(f'\\nðŸŽ‰ Goal Reached in {i_episode} episodes!')\n",
        "                    torch.save(agent.qnetwork_local.state_dict(), os.path.join(checkpoint_dir, f'{agent.mode}_solved_{i_episode}.pth'))\n",
        "                    break\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\nâŒ ERROR in Episode {i_episode}:\")\n",
        "                print(f\"Exception: {type(e).__name__}: {e}\")\n",
        "                traceback.print_exc()\n",
        "\n",
        "                # Save emergency checkpoint\n",
        "                print(\"Saving emergency checkpoint...\")\n",
        "                torch.save({\n",
        "                    'episode': i_episode,\n",
        "                    'model_state_dict': agent.qnetwork_local.state_dict(),\n",
        "                    'scores': scores,\n",
        "                    'eps': eps,\n",
        "                }, os.path.join(checkpoint_dir, f'emergency_ep{i_episode}.pth'))\n",
        "\n",
        "                # Try to continue or break\n",
        "                user_input = input(\"Continue training? (y/n): \")\n",
        "                if user_input.lower() != 'y':\n",
        "                    break\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n\\nâš ï¸  Training interrupted by user\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n\\nâŒ FATAL ERROR:\")\n",
        "        print(f\"Exception: {type(e).__name__}: {e}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "    finally:\n",
        "        # Always save what we have\n",
        "        print(\"\\n\\nSaving final results...\")\n",
        "        ALL_SCORES[f'{agent.mode}_optimized'] = scores  # Use unique key\n",
        "        np.save(os.path.join(checkpoint_dir, 'dqn_project_scores.npy'), ALL_SCORES)\n",
        "        print(f\"âœ“ Saved {len(scores)} episodes\")\n",
        "        print(f\"âœ“ Best average score achieved: {best_avg_score:.2f}\")\n",
        "\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThBy0kRg0uUx"
      },
      "outputs": [],
      "source": [
        "# Initialize an empty dictionary to hold scores from all runs\n",
        "ALL_SCORES = {}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Resuming from checkpoint if it exists\n",
        "# Auto-resume from latest checkpoint\n",
        "\n",
        "checkpoint_files = glob.glob(os.path.join(checkpoint_dir, f'checkpoint_{CONFIG[\"MODE\"]}_ep*.pth'))\n",
        "\n",
        "if checkpoint_files:\n",
        "    # Sort to get the latest checkpoint\n",
        "    checkpoint_files.sort(key=lambda x: int(x.split('_ep')[-1].split('.')[0]))\n",
        "    latest_checkpoint = checkpoint_files[-1]\n",
        "\n",
        "    print(f\"\\nðŸ”„ Found existing checkpoint: {os.path.basename(latest_checkpoint)}\")\n",
        "    print(f\"ðŸ“ Location: {checkpoint_dir}\")\n",
        "\n",
        "    # Option to resume or start fresh\n",
        "    resume = input(\"Resume from checkpoint? (y/n): \").lower() == 'y'\n",
        "\n",
        "    if resume:\n",
        "        print(f\"Loading checkpoint...\")\n",
        "        checkpoint = torch.load(latest_checkpoint)\n",
        "\n",
        "        agent.qnetwork_local.load_state_dict(checkpoint['model_state_dict'])\n",
        "        agent.qnetwork_target.load_state_dict(checkpoint['target_state_dict'])\n",
        "        agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "        # Load previous scores\n",
        "        scores_path = os.path.join(checkpoint_dir, 'dqn_project_scores.npy')\n",
        "        if os.path.exists(scores_path):\n",
        "            ALL_SCORES = np.load(scores_path, allow_pickle=True).item()\n",
        "\n",
        "        start_episode = checkpoint['episode']\n",
        "        start_eps = checkpoint['eps']\n",
        "\n",
        "        print(f\"âœ“ Resumed from episode {start_episode}\")\n",
        "        print(f\"âœ“ Previous scores loaded: {len(checkpoint['scores'])} episodes\")\n",
        "        print(f\"âœ“ Epsilon: {start_eps:.4f}\")\n",
        "        if len(checkpoint['scores']) >= 100:\n",
        "            print(f\"âœ“ Last 100 episodes avg: {np.mean(checkpoint['scores'][-100:]):.2f}\")\n",
        "\n",
        "        # Update CONFIG to continue from where we left off\n",
        "        CONFIG['EPS_START'] = start_eps\n",
        "    else:\n",
        "        print(\"Starting fresh training...\")\n",
        "else:\n",
        "    print(\"No checkpoint found. Starting fresh training...\")"
      ],
      "metadata": {
        "id": "HTnFGVp3Bp9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# PRE-TRAINING VERIFICATION\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"SYSTEM CHECK BEFORE TRAINING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 1. Check RAM\n",
        "mem = psutil.virtual_memory()\n",
        "print(f\"\\nðŸ“Š RAM Status:\")\n",
        "print(f\"   Total: {mem.total / (1024**3):.2f} GB\")\n",
        "print(f\"   Available: {mem.available / (1024**3):.2f} GB\")\n",
        "print(f\"   Used: {mem.used / (1024**3):.2f} GB ({mem.percent}%)\")\n",
        "\n",
        "if mem.available / (1024**3) < 2.0:\n",
        "    print(\"   âš ï¸  WARNING: Less than 2GB RAM available!\")\n",
        "else:\n",
        "    print(\"   âœ“ RAM looks good\")\n",
        "\n",
        "# 2. Check GPU\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\\nðŸŽ® GPU Status:\")\n",
        "    print(f\"   Device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Total Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB\")\n",
        "    print(f\"   Allocated: {torch.cuda.memory_allocated() / (1024**3):.2f} GB\")\n",
        "    print(f\"   Cached: {torch.cuda.memory_reserved() / (1024**3):.2f} GB\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸  No GPU detected - training will be VERY slow!\")\n",
        "\n",
        "# 3. Verify networks are on GPU\n",
        "print(f\"\\nðŸ§  Network Status:\")\n",
        "print(f\"   Local network device: {next(agent.qnetwork_local.parameters()).device}\")\n",
        "print(f\"   Target network device: {next(agent.qnetwork_target.parameters()).device}\")\n",
        "\n",
        "# 4. Check buffer configuration\n",
        "print(f\"\\nðŸ’¾ Replay Buffer:\")\n",
        "print(f\"   Max size: {CONFIG['BUFFER_SIZE']:,}\")\n",
        "print(f\"   Current size: {len(agent.memory)}\")\n",
        "print(f\"   Batch size: {CONFIG['BATCH_SIZE']}\")\n",
        "\n",
        "# Estimate memory usage\n",
        "frames_per_exp = 2 * 4 * 84 * 84  # state + next_state, 4 frames each\n",
        "if hasattr(agent.memory.memory, 'maxlen') and len(agent.memory) > 0:\n",
        "    # Check if using uint8 (1 byte) or float32 (4 bytes)\n",
        "    sample_exp = list(agent.memory.memory)[0]\n",
        "    dtype_size = 1 if sample_exp.state.dtype == np.uint8 else 4\n",
        "    estimated_mb = (CONFIG['BUFFER_SIZE'] * frames_per_exp * dtype_size) / (1024**2)\n",
        "    print(f\"   Data type: {sample_exp.state.dtype}\")\n",
        "    print(f\"   Estimated buffer memory: {estimated_mb:.0f} MB ({estimated_mb/1024:.2f} GB)\")\n",
        "\n",
        "    if dtype_size == 4:\n",
        "        print(\"   âš ï¸  WARNING: Using float32! Switch to uint8 to save 75% memory\")\n",
        "    else:\n",
        "        print(\"   âœ“ Using uint8 compression\")\n",
        "else:\n",
        "    # Empty buffer estimation\n",
        "    estimated_mb_uint8 = (CONFIG['BUFFER_SIZE'] * frames_per_exp * 1) / (1024**2)\n",
        "    estimated_mb_float32 = (CONFIG['BUFFER_SIZE'] * frames_per_exp * 4) / (1024**2)\n",
        "    print(f\"   Estimated memory (uint8): {estimated_mb_uint8:.0f} MB ({estimated_mb_uint8/1024:.2f} GB)\")\n",
        "    print(f\"   Estimated memory (float32): {estimated_mb_float32:.0f} MB ({estimated_mb_float32/1024:.2f} GB)\")\n",
        "\n",
        "# 5. Config summary\n",
        "print(f\"\\nâš™ï¸  Training Config:\")\n",
        "print(f\"   Mode: {CONFIG['MODE']}\")\n",
        "print(f\"   Episodes: {CONFIG['N_EPISODES']}\")\n",
        "print(f\"   Buffer size: {CONFIG['BUFFER_SIZE']:,}\")\n",
        "print(f\"   Epsilon decay: {CONFIG['EPS_DECAY']}\")\n",
        "print(f\"   Checkpoint dir: {checkpoint_dir}\")\n",
        "\n",
        "# 6. Final recommendation\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "total_estimated_gb = estimated_mb_uint8 / 1024 if 'estimated_mb_uint8' in locals() else 1.5\n",
        "ram_available = mem.available / (1024**3)\n",
        "\n",
        "if ram_available > total_estimated_gb + 2:  # +2GB safety margin\n",
        "    print(\"âœ… READY TO TRAIN - Sufficient resources available\")\n",
        "else:\n",
        "    print(\"âš ï¸  RISK OF CRASH - Consider:\")\n",
        "    print(f\"   - Reduce BUFFER_SIZE to {int(CONFIG['BUFFER_SIZE'] * 0.5):,}\")\n",
        "    print(\"   - Make sure uint8 ReplayBuffer is being used\")\n",
        "    print(\"   - Close other applications\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Force garbage collection before starting\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "KdgHNirnEtOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this right after creating the agent\n",
        "print(\"\\nðŸ” Testing ReplayBuffer compression:\")\n",
        "\n",
        "# Create a test state\n",
        "test_state = np.random.rand(4, 84, 84).astype(np.float32)\n",
        "test_action = 0\n",
        "test_reward = 1.0\n",
        "test_next_state = np.random.rand(4, 84, 84).astype(np.float32)\n",
        "test_done = False\n",
        "\n",
        "# Add to buffer\n",
        "agent.memory.add(test_state, test_action, test_reward, test_next_state, test_done)\n",
        "\n",
        "# Check what was stored\n",
        "if len(agent.memory) > 0:\n",
        "    stored_exp = list(agent.memory.memory)[0]\n",
        "    print(f\"Stored state dtype: {stored_exp.state.dtype}\")\n",
        "    print(f\"Stored state shape: {stored_exp.state.shape}\")\n",
        "\n",
        "    if stored_exp.state.dtype == np.uint8:\n",
        "        print(\"âœ… ReplayBuffer IS using uint8 compression\")\n",
        "    else:\n",
        "        print(\"âŒ ReplayBuffer NOT using uint8! This will cause crashes!\")\n",
        "        print(\"You need to use the uint8 ReplayBuffer implementation!\")"
      ],
      "metadata": {
        "id": "Ja7B1cHcGLiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P87GiE140uUx"
      },
      "outputs": [],
      "source": [
        "# Run training\n",
        "scores = dqn_train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kU5ktOld0uUy"
      },
      "outputs": [],
      "source": [
        "def plot_all_dqn_scores(all_scores_dict, window=100):\n",
        "    \"\"\"\n",
        "    Loads scores for all DQN variants and plots their moving average on a single graph.\n",
        "\n",
        "    Args:\n",
        "        all_scores_dict (dict): Dictionary mapping mode names ('SimpleDQN', etc.) to lists of episode scores.\n",
        "        window (int): The window size for the moving average.\n",
        "    \"\"\"\n",
        "    if not all_scores_dict:\n",
        "        print(\"No scores available to plot. Please run training for at least one agent.\")\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    for mode, scores in all_scores_dict.items():\n",
        "        if len(scores) >= window:\n",
        "            # Calculate 100-episode moving average\n",
        "            moving_avg = np.convolve(scores, np.ones(window)/window, mode='valid')\n",
        "\n",
        "            # The x-axis should start at the window size, as the moving average starts there\n",
        "            x_axis = np.arange(len(moving_avg)) + window\n",
        "\n",
        "            plt.plot(x_axis, moving_avg, label=f'{mode} (Avg={moving_avg[-1]:.2f})')\n",
        "        else:\n",
        "            print(f\"Not enough data to calculate moving average for {mode}.\")\n",
        "\n",
        "    # Add score targets as horizontal lines, similar to the presentation graph\n",
        "    plt.axhline(y=400, color='r', linestyle='--', linewidth=1, label='Goal: 400')\n",
        "    plt.axhline(y=500, color='g', linestyle='--', linewidth=1, label='Goal: 500')\n",
        "\n",
        "    plt.title('Consolidated DQN Training Progress (100-Episode Moving Average)')\n",
        "    plt.ylabel('Average Score (100-Game Window)')\n",
        "    plt.xlabel('Episode #')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.savefig('all_dqn_scores.png')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jC9Nqis0uUy"
      },
      "outputs": [],
      "source": [
        "# Run run AFTER you have completed at least one training run\n",
        "try:\n",
        "    # Load saved scores (if they exist from previous runs)\n",
        "    scores_path = '/content/drive/MyDrive/DQN_Checkpoints/dqn_project_scores.npy'\n",
        "    loaded_scores = np.load(scores_path, allow_pickle=True).item()\n",
        "    plot_all_dqn_scores(loaded_scores)\n",
        "except FileNotFoundError:\n",
        "    print(\"Scores file not found. Run training first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "video_display"
      },
      "source": [
        "## 6. Video Visualization Utility\n",
        "This function is provided for optional video recording using a trained model's weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "video_code"
      },
      "outputs": [],
      "source": [
        "def render_mp4(videopath: str) -> str:\n",
        "  \"\"\"Gets a string containing a b64-encoded version of the MP4 video.\"\"\"\n",
        "  import os\n",
        "  if not os.path.exists(videopath):\n",
        "      return f\"<p>Video file not found at {videopath}. Run a test episode first.</p>\"\n",
        "\n",
        "  mp4 = open(videopath, 'rb').read()\n",
        "  base64_encoded_mp4 = b64encode(mp4).decode()\n",
        "  return f'<video width=400 controls><source src=\"data:video/mp4;base64,{base64_encoded_mp4}\" type=\"video/mp4\"></video>'\n",
        "\n",
        "def run_and_record(env_id, weights_path, mode, seed=CONFIG['SEED'], num_episodes=1):\n",
        "    \"\"\"Runs a specified agent on the environment and records the interaction.\"\"\"\n",
        "\n",
        "    # 1. Setup Environment\n",
        "    # Use the same wrapper stack as training for consistent state representation\n",
        "    env_render = make_atari_env(env_id, seed=seed)\n",
        "\n",
        "    # 2. Setup Agent\n",
        "    action_size = env_render.action_space.n\n",
        "    state_shape = env_render.observation_space.shape\n",
        "\n",
        "    # Use the appropriate Agent class\n",
        "    if mode == \"SimpleDQN\":\n",
        "        test_agent = SimpleDQNAgent(state_shape, action_size, seed)\n",
        "    elif mode == \"DoubleDQN\":\n",
        "        test_agent = DoubleDQNAgent(state_shape, action_size, seed)\n",
        "    elif mode == \"DuelingDQN\":\n",
        "        test_agent = DuelingDQNAgent(state_shape, action_size, seed)\n",
        "    else:\n",
        "        return f\"<p>Invalid MODE specified for testing: {mode}</p>\"\n",
        "\n",
        "    # 3. Load Weights\n",
        "    try:\n",
        "        test_agent.qnetwork_local.load_state_dict(torch.load(weights_path, map_location=device))\n",
        "        test_agent.qnetwork_local.eval()\n",
        "        print(f\"Successfully loaded {mode} weights from {weights_path}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Checkpoint file {weights_path} not found. Agent will use random weights.\")\n",
        "        return\n",
        "\n",
        "    # 4. Record Episodes\n",
        "    video_path = f'{mode}_{env_id.split(\"/\")[-1]}_test.mp4'\n",
        "    frames = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, info = env_render.reset(seed=seed)\n",
        "        score = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # The FrameStack wrapper returns a LazyFrame, convert to NumPy array\n",
        "            state_np = np.array(state)\n",
        "            action = test_agent.act(state_np, eps=0.0)\n",
        "\n",
        "            # Capture frame (convert to RGB before saving)\n",
        "            frames.append(env_render.render())\n",
        "\n",
        "            next_state, reward, terminated, truncated, info = env_render.step(action)\n",
        "            done = terminated or truncated\n",
        "            state = next_state\n",
        "            score += reward\n",
        "\n",
        "        print(f\"Test Episode {episode+1} score: {score:.2f}\")\n",
        "\n",
        "    env_render.close()\n",
        "\n",
        "    # Save video\n",
        "    imageio.mimsave(video_path, frames, fps=30)\n",
        "\n",
        "    # Display video\n",
        "    html = render_mp4(video_path)\n",
        "    ipythondisplay.display(ipythondisplay.HTML(html))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FJMkbNG0uUy"
      },
      "outputs": [],
      "source": [
        "# Example usage (Uncomment and update weights_path after training):\n",
        "# run_and_record(CONFIG['ENV_ID'], '/content/drive/MyDrive/DQN_Checkpoints/SimpleDQN_BEST.pth', 'SimpleDQN', num_episodes=1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "name": "AIDL_B02_DQN_SpaceInvaders_Variants_Clean.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}