{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/tcharos/AIDL_B02-Advanced-Topics-in-Deep-Learning/blob/main/agent_based_dqn_space_invaders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent-Based DQN for Space Invaders\n",
    "\n",
    "Implementation of DQN variants using Agent class with hard target updates:\n",
    "- Standard DQN\n",
    "- Double DQN\n",
    "- Duelling DQN\n",
    "- Prioritized Experience Replay (PER)\n",
    "- Learning Rate Scheduling\n",
    "- Graceful Interruption (Ctrl+C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: gymnasium[atari,accept-rom-license]\n",
      "Requirement already satisfied: ale-py in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (0.11.2)\n",
      "Requirement already satisfied: numpy>1.20 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from ale-py) (1.26.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.10.0 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from ale-py) (7.1.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from ale-py) (4.11.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from importlib-metadata>=4.10.0->ale-py) (3.18.1)\n",
      "Requirement already satisfied: torch in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (2.8.0)\n",
      "Requirement already satisfied: scipy in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (1.13.0)\n",
      "Requirement already satisfied: numpy in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (1.26.4)\n",
      "Requirement already satisfied: psutil in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (5.9.8)\n",
      "Requirement already satisfied: filelock in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: opencv-python in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (4.11.0.86)\n",
      "Requirement already satisfied: matplotlib in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (3.8.4)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from opencv-python) (1.26.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from matplotlib) (6.4.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.18.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium[atari,accept-rom-license]\n",
    "!pip install ale-py\n",
    "!pip install torch scipy numpy psutil\n",
    "!pip install opencv-python matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and verify ALE is available\n",
    "import ale_py\n",
    "import gymnasium as gym\n",
    "gym.register_envs(ale_py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "from scipy.ndimage import zoom\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Apple Silicon)\n",
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Set device (MPS for Mac, CUDA for GPU, CPU otherwise)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Apple Silicon)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Drive Setup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoints will be saved locally to: ./checkpoints\n",
      "Best models will be saved locally to: ./best_models\n"
     ]
    }
   ],
   "source": [
    "USE_GDRIVE = False  # Set to True to enable Google Drive integration\n",
    "\n",
    "if USE_GDRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    CHECKPOINT_DIR = '/content/drive/MyDrive/DQN_SpaceInvaders_Checkpoints'\n",
    "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "    print(f\"Checkpoints will be saved to: {CHECKPOINT_DIR}\")\n",
    "else:\n",
    "    CHECKPOINT_DIR = './checkpoints'\n",
    "    BEST_MODELS = './best_models'\n",
    "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "    os.makedirs(BEST_MODELS, exist_ok=True)\n",
    "    print(f\"Checkpoints will be saved locally to: {CHECKPOINT_DIR}\")\n",
    "    print(f\"Best models will be saved locally to: {BEST_MODELS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(config, policy_net, target_net, optimizer, episode, avg_score, \n",
    "                   episode_rewards, is_best=False):\n",
    "    \"\"\"Save model checkpoint\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    dqn_type = config['DQN_TYPE']\n",
    "    per_suffix = \"_PER\" if config['USE_PER'] else \"\"\n",
    "    \n",
    "    if is_best:\n",
    "        filename = f\"{dqn_type}{per_suffix}_best.pth\"\n",
    "        filepath = os.path.join(config['BEST_MODELS_DIR'], filename)\n",
    "        print(f'\\t\\tNew best avg: {avg_score:.2f} - saved to {filename}')\n",
    "    else:\n",
    "        filename = f\"{dqn_type}{per_suffix}_ep{episode}.pth\"\n",
    "        filepath = os.path.join(config['CHECKPOINT_DIR'], filename)\n",
    "        print(f'\\t\\tCheckpoint saved: {filename}')\n",
    "\n",
    "    torch.save({\n",
    "        'episode': episode,\n",
    "        'config': config,\n",
    "        'policy_net_state_dict': policy_net.state_dict(),\n",
    "        'target_net_state_dict': target_net.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'avg_score': avg_score,\n",
    "        'timestamp': timestamp\n",
    "    }, filepath)\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "def print_config(config):\n",
    "    \"\"\"Print configuration in a formatted way\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"  DQN TYPE: {config['DQN_TYPE']}\")\n",
    "    if config['USE_PER']:\n",
    "        print(f\"  Using Prioritized Experience Replay (PER)\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nConfiguration:\")\n",
    "    print(\"-\"*70)\n",
    "    for key, value in config.items():\n",
    "        if key not in ['CHECKPOINT_DIR']:  # Skip long paths\n",
    "            print(f\"  {key:20s}: {value}\")\n",
    "    print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"Standard DQN network.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "    \n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class DuellingDQN(nn.Module):\n",
    "    \"\"\"Duelling DQN network - separates value and advantage streams.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DuellingDQN, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        \n",
    "        # Value stream\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "        \n",
    "        # Advantage stream\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "    \n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        value = self.value_stream(x)\n",
    "        advantage = self.advantage_stream(x)\n",
    "        \n",
    "        # Combine value and advantage: Q(s,a) = V(s) + (A(s,a) - mean(A))\n",
    "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Standard uniform replay buffer.\"\"\"\n",
    "    \n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(next_states),\n",
    "            np.array(dones, dtype=np.float32)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    \"\"\"Prioritized Experience Replay buffer.\"\"\"\n",
    "    \n",
    "    def __init__(self, buffer_size, alpha=0.6, beta_start=0.4, beta_frames=100000, epsilon=1e-6):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "        self.priorities = deque(maxlen=buffer_size)\n",
    "        self.alpha = alpha\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_frames = beta_frames\n",
    "        self.epsilon = epsilon\n",
    "        self.frame = 0\n",
    "        self.max_priority = 1.0\n",
    "    \n",
    "    def beta_schedule(self):\n",
    "        \"\"\"Linearly increase beta from beta_start to 1.0.\"\"\"\n",
    "        return min(1.0, self.beta_start + self.frame * (1.0 - self.beta_start) / self.beta_frames)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "        self.priorities.append(float(self.max_priority))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        self.frame += 1\n",
    "        beta = self.beta_schedule()\n",
    "        \n",
    "        priorities = np.array(self.priorities)\n",
    "        probs = priorities ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs, replace=False)\n",
    "        \n",
    "        # Importance sampling weights\n",
    "        weights = (len(self.buffer) * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        \n",
    "        batch = [self.buffer[idx] for idx in indices]\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(next_states),\n",
    "            np.array(dones, dtype=np.float32),\n",
    "            indices,\n",
    "            weights.astype(np.float32)\n",
    "        )\n",
    "    \n",
    "    def update_priorities(self, indices, td_errors):\n",
    "        \"\"\"Update priorities based on TD errors.\"\"\"\n",
    "        td_errors = td_errors.flatten()\n",
    "        for idx, error in zip(indices, td_errors):\n",
    "            priority = (abs(error) + self.epsilon) ** self.alpha\n",
    "            self.priorities[idx] = float(priority) \n",
    "            self.max_priority = max(self.max_priority, float(priority))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    \"\"\"Preprocess Space Invaders frame: grayscale + resize to 84x84.\"\"\"\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "    resized = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "    normalized = resized / 255.0\n",
    "    return normalized.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Class with Hard Target Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"DQN Agent with hard target network updates.\"\"\"\n",
    "    \n",
    "    def __init__(self, config, device='cpu'):\n",
    "        \"\"\"\n",
    "        Initialize DQN Agent.\n",
    "        \n",
    "        Args:\n",
    "            config (dict): Configuration dictionary\n",
    "            device (str): Device to use ('cpu', 'cuda', 'mps')\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.n_actions = config['N_ACTIONS']\n",
    "        \n",
    "        # Create networks\n",
    "        input_shape = (config['N_FRAMES'], 84, 84)\n",
    "        if config['DQN_TYPE'] == 'DuellingDQN':\n",
    "            self.policy_net = DuellingDQN(input_shape, self.n_actions).to(device)\n",
    "            self.target_net = DuellingDQN(input_shape, self.n_actions).to(device)\n",
    "        else:\n",
    "            self.policy_net = DQN(input_shape, self.n_actions).to(device)\n",
    "            self.target_net = DQN(input_shape, self.n_actions).to(device)\n",
    "        \n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=config['LEARNING_RATE'])\n",
    "        \n",
    "        # LR Scheduler (optional)\n",
    "        self.scheduler = None\n",
    "        if config.get('LR_SCHEDULER', False):\n",
    "            self.scheduler = ExponentialLR(self.optimizer, gamma=config.get('LR_GAMMA', 0.9995))\n",
    "        \n",
    "        # Replay buffer\n",
    "        if config.get('USE_PER', False):\n",
    "            self.memory = PrioritizedReplayBuffer(\n",
    "                config['BUFFER_SIZE'],\n",
    "                alpha=config.get('PER_ALPHA', 0.6),\n",
    "                beta_start=config.get('PER_BETA_START', 0.4),\n",
    "                beta_frames=config.get('PER_BETA_FRAMES', 100000),\n",
    "                epsilon=config.get('PER_EPSILON', 1e-6)\n",
    "            )\n",
    "        else:\n",
    "            self.memory = ReplayBuffer(config['BUFFER_SIZE'])\n",
    "        \n",
    "        self.steps = 0\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        \"\"\"Select action using epsilon-greedy policy.\"\"\"\n",
    "        if random.random() < epsilon:\n",
    "            return random.randrange(self.n_actions)\n",
    "        else:\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.policy_net(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience and learn.\"\"\"\n",
    "        # Store experience\n",
    "        self.memory.push(state, action, reward, next_state, done)\n",
    "        self.steps += 1\n",
    "        \n",
    "        # Learn from experience\n",
    "        if len(self.memory) >= self.config['BATCH_SIZE']:\n",
    "            self.learn()\n",
    "        \n",
    "        # Hard update target network\n",
    "        if self.steps % self.config['TARGET_UPDATE'] == 0:\n",
    "            self.update_target_network()\n",
    "    \n",
    "    def learn(self):\n",
    "        \"\"\"Update policy network using batch of experiences.\"\"\"\n",
    "        # Sample from memory\n",
    "        use_per = self.config.get('USE_PER', False)\n",
    "        \n",
    "        if use_per:\n",
    "            states, actions, rewards, next_states, dones, indices, weights = self.memory.sample(self.config['BATCH_SIZE'])\n",
    "        else:\n",
    "            states, actions, rewards, next_states, dones = self.memory.sample(self.config['BATCH_SIZE'])\n",
    "            weights = np.ones(self.config['BATCH_SIZE'])\n",
    "            indices = None\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1).to(self.device)\n",
    "        weights = torch.FloatTensor(weights).unsqueeze(1).to(self.device)\n",
    "        \n",
    "        # Get current Q values\n",
    "        current_q_values = self.policy_net(states).gather(1, actions)\n",
    "        \n",
    "        # Compute target Q values\n",
    "        with torch.no_grad():\n",
    "            if self.config['DQN_TYPE'] == 'DoubleDQN':\n",
    "                # Double DQN: use policy net to select actions, target net to evaluate\n",
    "                next_actions = self.policy_net(next_states).argmax(1, keepdim=True)\n",
    "                next_q_values = self.target_net(next_states).gather(1, next_actions)\n",
    "            else:\n",
    "                # Standard DQN (or Duelling DQN)\n",
    "                next_q_values = self.target_net(next_states).max(1, keepdim=True)[0]\n",
    "            \n",
    "            target_q_values = rewards + (1 - dones) * self.config['GAMMA'] * next_q_values\n",
    "        \n",
    "        # Compute loss\n",
    "        td_errors = target_q_values - current_q_values\n",
    "        loss = (td_errors.pow(2) * weights).mean()\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), max_norm=10.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update priorities if using PER\n",
    "        if use_per and indices is not None:\n",
    "            self.memory.update_priorities(indices, td_errors.detach().cpu().numpy())\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Hard update: copy policy network weights to target network.\"\"\"\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "    \n",
    "    def get_epsilon(self, episode_steps):\n",
    "        \"\"\"Calculate current epsilon based on decay schedule.\"\"\"\n",
    "        epsilon = self.config['EPSILON_END'] + (self.config['EPSILON_START'] - self.config['EPSILON_END']) * \\\n",
    "                  np.exp(-1. * episode_steps / self.config['EPSILON_DECAY'])\n",
    "        return epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(config, agent, device='cpu'):\n",
    "    \"\"\"Train DQN agent with graceful interruption support.\"\"\"\n",
    "    \n",
    "    # Create environment\n",
    "    env = gym.make(config['ENV_ID'])\n",
    "    if config.get('SEED') is not None:\n",
    "        env.reset(seed=config['SEED'])\n",
    "        random.seed(config['SEED'])\n",
    "        np.random.seed(config['SEED'])\n",
    "        torch.manual_seed(config['SEED'])\n",
    "    \n",
    "    episode_rewards = []\n",
    "    best_avg_score = -float('inf')\n",
    "    \n",
    "    # Print configuration\n",
    "    print_config(config)\n",
    "    \n",
    "    if agent.scheduler:\n",
    "        print(f\"\\n‚úÖ LR Scheduler enabled: ExponentialLR (gamma={config.get('LR_GAMMA', 0.9995)})\")\n",
    "    \n",
    "    start_time = datetime.now() \n",
    "\n",
    "    print(\"\\nüí° Press Ctrl+C anytime to stop training and save progress\\n\")\n",
    "    print(f\"--- TRAINING STARTED: {start_time.strftime('%Y-%m-%d %H:%M:%S')} ---\\n\")\n",
    "    \n",
    "    try:\n",
    "        for episode in range(config['N_EPISODES']):\n",
    "            state, _ = env.reset()\n",
    "            state = preprocess_frame(state)\n",
    "            state_stack = deque([state] * config['N_FRAMES'], maxlen=config['N_FRAMES'])\n",
    "            \n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                # Get epsilon and select action\n",
    "                epsilon = agent.get_epsilon(agent.steps)\n",
    "                state_array = np.array(state_stack)\n",
    "                action = agent.act(state_array, epsilon)\n",
    "                \n",
    "                # Take step\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                next_state = preprocess_frame(next_state)\n",
    "                next_state_stack = state_stack.copy()\n",
    "                next_state_stack.append(next_state)\n",
    "                \n",
    "                # Agent step (store and learn)\n",
    "                agent.step(\n",
    "                    np.array(state_stack),\n",
    "                    action,\n",
    "                    reward,\n",
    "                    np.array(next_state_stack),\n",
    "                    float(done)\n",
    "                )\n",
    "                \n",
    "                state_stack = next_state_stack\n",
    "                episode_reward += reward\n",
    "            \n",
    "            episode_rewards.append(episode_reward)\n",
    "            \n",
    "            # Step LR scheduler\n",
    "            if agent.scheduler:\n",
    "                agent.scheduler.step()\n",
    "            \n",
    "            # Print progress\n",
    "            # Print every 10 episodes the progress\n",
    "            if episode % 10 == 0:\n",
    "                avg_score = np.mean(episode_rewards[-100:]) if len(episode_rewards) >= 100 else np.mean(episode_rewards)\n",
    "                if avg_score > best_avg_score:\n",
    "                    best_avg_score = avg_score\n",
    "                    save_checkpoint(\n",
    "                        config, agent.policy_net, agent.target_net, agent.optimizer, \n",
    "                        episode, avg_score, episode_rewards, is_best=True)\n",
    "                epsilon = agent.get_epsilon(agent.steps)\n",
    "                current_lr = agent.optimizer.param_groups[0]['lr']\n",
    "                print(f'Episode {episode} | Score: {episode_reward:.1f} | Avg: {avg_score:.2f} | Eps: {epsilon:.3f} | LR: {current_lr:.6f} | Steps: {agent.steps}')\n",
    "\n",
    "            if episode % config['CHECKPOINT_EVERY'] == 0:\n",
    "                save_checkpoint(\n",
    "                    config, agent.policy_net, agent.target_net, agent.optimizer, \n",
    "                    episode, avg_score, episode_rewards, is_best=False)\n",
    "            \n",
    "            if episode % 40 == 0:\n",
    "                mem = psutil.virtual_memory()\n",
    "                gpu_mem_str = \"N/A\"\n",
    "                if torch.cuda.is_available():\n",
    "                    gpu_mem = torch.cuda.memory_allocated() / 1024**3\n",
    "                    gpu_mem_str = f\"{gpu_mem:.2f}GB\"\n",
    "                elif torch.backends.mps.is_available() and device.type == 'mps':\n",
    "                    gpu_mem_str = \"Active\"\n",
    "                print(f'\\t\\tRAM: {mem.percent:.1f}% | GPU: {gpu_mem_str} | Buffer: {len(agent.memory)}/{config[\"BUFFER_SIZE\"]}')\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        end_time = datetime.now()\n",
    "        duration = end_time - start_time\n",
    "        \n",
    "        print(\"\\n\\n\" + \"=\"*70)\n",
    "        print(\"‚ö†Ô∏è  TRAINING INTERRUPTED BY USER\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        if len(episode_rewards) > 0:\n",
    "            current_episode = len(episode_rewards) - 1\n",
    "            avg_score = np.mean(episode_rewards[-100:]) if len(episode_rewards) >= 100 else np.mean(episode_rewards)\n",
    "            \n",
    "            print(f\"\\nüìä Training Statistics at Interruption:\")\n",
    "            print(\"-\" * 70)\n",
    "            print(f\"  Episodes completed: {current_episode + 1} / {config['N_EPISODES']}\")\n",
    "            print(f\"  Total steps: {agent.steps:,}\")\n",
    "            print(f\"  Last episode score: {episode_rewards[-1]:.1f}\")\n",
    "            print(f\"  Average score (last {min(100, len(episode_rewards))} episodes): {avg_score:.2f}\")\n",
    "            print(f\"  Best average score: {best_avg_score:.2f}\")\n",
    "            print(f\"  Max episode score: {max(episode_rewards):.1f}\")\n",
    "            print(f\"  Min episode score: {min(episode_rewards):.1f}\")\n",
    "            \n",
    "            # --- INTERRUPTED TIME LOGGING ---\n",
    "            print(f\"\\n--- INTERRUPTED AT: {end_time.strftime('%Y-%m-%d %H:%M:%S')} ---\")\n",
    "            print(f\"--- DURATION: {duration} ---\")\n",
    "            # --------------------------------\n",
    "            \n",
    "            save_checkpoint(\n",
    "                config, agent.policy_net, agent.target_net, agent.optimizer, \n",
    "                current_episode, avg_score, episode_rewards, is_best=False)\n",
    "            \n",
    "            try:\n",
    "                import matplotlib.pyplot as plt\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                plt.plot(episode_rewards, alpha=0.6, label='Episode Reward')\n",
    "                if len(episode_rewards) >= 10:\n",
    "                    window = min(100, len(episode_rewards))\n",
    "                    moving_avg = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\n",
    "                    plt.plot(range(window-1, len(episode_rewards)), moving_avg, \n",
    "                            label=f'Moving Avg ({window})', linewidth=2, color='red')\n",
    "                plt.axhline(y=avg_score, color='green', linestyle='--', \n",
    "                           label=f'Current Avg: {avg_score:.2f}')\n",
    "                plt.xlabel('Episode')\n",
    "                plt.ylabel('Score')\n",
    "                plt.title(f'Training Progress (Interrupted at Episode {current_episode})')\n",
    "                plt.legend()\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                plt.tight_layout()\n",
    "                \n",
    "                plot_filename = f'interrupted_plot_{timestamp}.png'\n",
    "                plot_filepath = os.path.join(config['BEST_MODELS_DIR'], plot_filename)\n",
    "                plt.savefig(plot_filepath, dpi=100)\n",
    "                \n",
    "                print(f\"üìà Plot saved: {plot_filepath}\")\n",
    "                plt.close()\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Could not generate plot: {e}\")\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"‚úÖ All progress saved successfully!\")\n",
    "            print(\"üí° You can resume training by loading the checkpoint\")\n",
    "            print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    finally:\n",
    "        env.close()\n",
    "    \n",
    "    if len(episode_rewards) == config['N_EPISODES']:\n",
    "        # --- END TIME LOGGING ---\n",
    "        end_time = datetime.now()\n",
    "        duration = end_time - start_time\n",
    "        print(f\"\\n--- TRAINING FINISHED: {end_time.strftime('%Y-%m-%d %H:%M:%S')} ---\")\n",
    "        print(f\"--- DURATION: {duration} ---\")\n",
    "        # ------------------------\n",
    "        \n",
    "        print(\"\\n‚úÖ Training completed successfully!\")\n",
    "        print(f\"Best average score: {best_avg_score:.2f}\\n\")\n",
    "    \n",
    "    return episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Configuration\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    'ENV_ID': 'ALE/SpaceInvaders-v5',\n",
    "    'SEED': 18,\n",
    "    'N_FRAMES': 4,\n",
    "    'N_ACTIONS': 6,\n",
    "    'N_EPISODES': 6000,\n",
    "    'LEARNING_RATE': 0.00001,\n",
    "    'GAMMA': 0.99,\n",
    "    'BATCH_SIZE': 32,\n",
    "    'EPSILON_START': 1.0,\n",
    "    'EPSILON_END': 0.1,\n",
    "    'EPSILON_DECAY': 100000,\n",
    "    'BUFFER_SIZE': 150000,\n",
    "    'TARGET_UPDATE': 10000,\n",
    "    'CHECKPOINT_EVERY': 400,\n",
    "    'USE_GDRIVE': USE_GDRIVE,\n",
    "    'CHECKPOINT_DIR': CHECKPOINT_DIR,\n",
    "    'BEST_MODELS_DIR': BEST_MODELS,\n",
    "    'DQN_TYPE': 'DQN',  # Options: 'DQN', 'DoubleDQN', 'DuellingDQN'\n",
    "    'USE_PER': False, \n",
    "    'LR_SCHEDULER': False,\n",
    "    'LR_GAMMA': 0.999, \n",
    "    \n",
    "    # PER hyperparameters (if USE_PER=True)\n",
    "    'PER_ALPHA': 0.6,\n",
    "    'PER_BETA_START': 0.4,\n",
    "    'PER_BETA_FRAMES': 100000,\n",
    "    'PER_EPSILON': 1e-6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple DQN agent\n",
    "\n",
    "CONFIG_DQN = BASE_CONFIG.copy()\n",
    "CONFIG_DQN['DQN_TYPE'] = 'DQN'\n",
    "CONFIG_DQN['USE_PER'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(CONFIG_DQN, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = train_agent(CONFIG_DQN, agent, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Double DQN agent\n",
    "\n",
    "CONFIG_DDQN = BASE_CONFIG.copy()\n",
    "CONFIG_DDQN['DQN_TYPE'] = 'DoubleDQN'\n",
    "CONFIG_DDQN['N_EPISODES'] = 10000\n",
    "CONFIG_DDQN['LEARNING_RATE'] = 0.0001\n",
    "CONFIG_DDQN['EPSILON_DECAY'] = 1000000\n",
    "CONFIG_DDQN['LR_SCHEDULER'] = False\n",
    "CONFIG_DDQN['USE_PER'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_ddqn = DQNAgent(CONFIG_DDQN, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.11.2+ecc1138)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "  DQN TYPE: DoubleDQN\n",
      "  Using Prioritized Experience Replay (PER)\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "----------------------------------------------------------------------\n",
      "  ENV_ID              : ALE/SpaceInvaders-v5\n",
      "  SEED                : 18\n",
      "  N_FRAMES            : 4\n",
      "  N_ACTIONS           : 6\n",
      "  N_EPISODES          : 10000\n",
      "  LEARNING_RATE       : 0.0001\n",
      "  GAMMA               : 0.99\n",
      "  BATCH_SIZE          : 32\n",
      "  EPSILON_START       : 1.0\n",
      "  EPSILON_END         : 0.1\n",
      "  EPSILON_DECAY       : 1000000\n",
      "  BUFFER_SIZE         : 150000\n",
      "  TARGET_UPDATE       : 10000\n",
      "  CHECKPOINT_EVERY    : 400\n",
      "  USE_GDRIVE          : False\n",
      "  BEST_MODELS_DIR     : ./best_models\n",
      "  DQN_TYPE            : DoubleDQN\n",
      "  USE_PER             : True\n",
      "  LR_SCHEDULER        : False\n",
      "  LR_GAMMA            : 0.999\n",
      "  PER_ALPHA           : 0.6\n",
      "  PER_BETA_START      : 0.4\n",
      "  PER_BETA_FRAMES     : 100000\n",
      "  PER_EPSILON         : 1e-06\n",
      "======================================================================\n",
      "\n",
      "\n",
      "üí° Press Ctrl+C anytime to stop training and save progress\n",
      "\n",
      "--- TRAINING STARTED: 2025-12-02 01:41:25 ---\n",
      "\n",
      "Episode 0 | Score: 90.0 | Avg: 90.00 | Eps: 1.000 | LR: 0.000100 | Steps: 316\n",
      "\t\tNew best avg: 90.00 - saved to DoubleDQN_PER_20251202_014130_best.pth\n",
      "\t\tRAM: 63.6% | GPU: Active | Buffer: 316/150000\n",
      "Episode 10 | Score: 50.0 | Avg: 125.00 | Eps: 0.995 | LR: 0.000100 | Steps: 5411\n",
      "Episode 20 | Score: 30.0 | Avg: 129.29 | Eps: 0.991 | LR: 0.000100 | Steps: 10281\n",
      "Episode 30 | Score: 460.0 | Avg: 148.71 | Eps: 0.985 | LR: 0.000100 | Steps: 16442\n",
      "Episode 40 | Score: 210.0 | Avg: 150.98 | Eps: 0.981 | LR: 0.000100 | Steps: 21343\n",
      "\t\tRAM: 63.7% | GPU: Active | Buffer: 21343/150000\n",
      "Episode 50 | Score: 105.0 | Avg: 151.96 | Eps: 0.977 | LR: 0.000100 | Steps: 26339\n",
      "Episode 60 | Score: 60.0 | Avg: 148.36 | Eps: 0.972 | LR: 0.000100 | Steps: 31188\n",
      "Episode 70 | Score: 110.0 | Avg: 154.37 | Eps: 0.967 | LR: 0.000100 | Steps: 36829\n",
      "Episode 80 | Score: 330.0 | Avg: 151.42 | Eps: 0.963 | LR: 0.000100 | Steps: 41731\n",
      "\t\tRAM: 71.5% | GPU: Active | Buffer: 41731/150000\n",
      "Episode 90 | Score: 210.0 | Avg: 160.55 | Eps: 0.958 | LR: 0.000100 | Steps: 47942\n",
      "Episode 100 | Score: 210.0 | Avg: 162.45 | Eps: 0.953 | LR: 0.000100 | Steps: 53277\n",
      "Episode 110 | Score: 155.0 | Avg: 166.60 | Eps: 0.949 | LR: 0.000100 | Steps: 58923\n",
      "Episode 120 | Score: 130.0 | Avg: 169.95 | Eps: 0.944 | LR: 0.000100 | Steps: 63974\n",
      "\t\tRAM: 74.9% | GPU: Active | Buffer: 63974/150000\n",
      "Episode 130 | Score: 210.0 | Avg: 169.20 | Eps: 0.939 | LR: 0.000100 | Steps: 70027\n",
      "Episode 140 | Score: 50.0 | Avg: 166.80 | Eps: 0.935 | LR: 0.000100 | Steps: 74930\n",
      "Episode 150 | Score: 150.0 | Avg: 163.70 | Eps: 0.931 | LR: 0.000100 | Steps: 79541\n",
      "Episode 160 | Score: 135.0 | Avg: 162.75 | Eps: 0.928 | LR: 0.000100 | Steps: 83838\n",
      "\t\tRAM: 77.7% | GPU: Active | Buffer: 83838/150000\n",
      "Episode 170 | Score: 130.0 | Avg: 154.45 | Eps: 0.924 | LR: 0.000100 | Steps: 88448\n",
      "Episode 180 | Score: 85.0 | Avg: 158.90 | Eps: 0.919 | LR: 0.000100 | Steps: 94102\n",
      "Episode 190 | Score: 45.0 | Avg: 145.60 | Eps: 0.916 | LR: 0.000100 | Steps: 98335\n",
      "Episode 200 | Score: 340.0 | Avg: 144.40 | Eps: 0.912 | LR: 0.000100 | Steps: 103340\n",
      "\t\tRAM: 79.4% | GPU: Active | Buffer: 103340/150000\n",
      "Episode 210 | Score: 245.0 | Avg: 144.10 | Eps: 0.907 | LR: 0.000100 | Steps: 108493\n",
      "Episode 220 | Score: 105.0 | Avg: 140.15 | Eps: 0.903 | LR: 0.000100 | Steps: 113829\n",
      "Episode 230 | Score: 65.0 | Avg: 132.55 | Eps: 0.900 | LR: 0.000100 | Steps: 118012\n",
      "Episode 240 | Score: 120.0 | Avg: 134.50 | Eps: 0.896 | LR: 0.000100 | Steps: 123215\n",
      "\t\tRAM: 81.7% | GPU: Active | Buffer: 123215/150000\n",
      "Episode 250 | Score: 135.0 | Avg: 135.90 | Eps: 0.891 | LR: 0.000100 | Steps: 128775\n",
      "Episode 260 | Score: 50.0 | Avg: 139.00 | Eps: 0.887 | LR: 0.000100 | Steps: 133586\n",
      "Episode 270 | Score: 285.0 | Avg: 142.75 | Eps: 0.883 | LR: 0.000100 | Steps: 138700\n",
      "Episode 280 | Score: 135.0 | Avg: 141.25 | Eps: 0.880 | LR: 0.000100 | Steps: 143542\n",
      "\t\tRAM: 83.4% | GPU: Active | Buffer: 143542/150000\n",
      "Episode 290 | Score: 420.0 | Avg: 153.20 | Eps: 0.875 | LR: 0.000100 | Steps: 149045\n",
      "Episode 300 | Score: 55.0 | Avg: 148.30 | Eps: 0.872 | LR: 0.000100 | Steps: 153774\n",
      "Episode 310 | Score: 210.0 | Avg: 145.70 | Eps: 0.868 | LR: 0.000100 | Steps: 158867\n",
      "Episode 320 | Score: 260.0 | Avg: 149.20 | Eps: 0.864 | LR: 0.000100 | Steps: 163748\n",
      "\t\tRAM: 83.2% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 330 | Score: 75.0 | Avg: 155.75 | Eps: 0.860 | LR: 0.000100 | Steps: 168903\n",
      "Episode 340 | Score: 415.0 | Avg: 161.95 | Eps: 0.856 | LR: 0.000100 | Steps: 174778\n",
      "Episode 350 | Score: 315.0 | Avg: 164.60 | Eps: 0.851 | LR: 0.000100 | Steps: 180373\n",
      "Episode 360 | Score: 150.0 | Avg: 170.75 | Eps: 0.847 | LR: 0.000100 | Steps: 185867\n",
      "\t\tRAM: 82.8% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 370 | Score: 100.0 | Avg: 169.40 | Eps: 0.844 | LR: 0.000100 | Steps: 190553\n",
      "Episode 380 | Score: 120.0 | Avg: 168.80 | Eps: 0.840 | LR: 0.000100 | Steps: 195430\n",
      "Episode 390 | Score: 155.0 | Avg: 160.20 | Eps: 0.837 | LR: 0.000100 | Steps: 199820\n",
      "Episode 400 | Score: 135.0 | Avg: 166.30 | Eps: 0.834 | LR: 0.000100 | Steps: 204353\n",
      "\t\tNew best avg: 166.30 - saved to DoubleDQN_PER_20251202_030023_best.pth\n",
      "\t\tRAM: 83.3% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 410 | Score: 90.0 | Avg: 168.65 | Eps: 0.830 | LR: 0.000100 | Steps: 209269\n",
      "Episode 420 | Score: 115.0 | Avg: 171.45 | Eps: 0.826 | LR: 0.000100 | Steps: 214641\n",
      "Episode 430 | Score: 120.0 | Avg: 169.05 | Eps: 0.823 | LR: 0.000100 | Steps: 219547\n",
      "Episode 440 | Score: 285.0 | Avg: 166.20 | Eps: 0.819 | LR: 0.000100 | Steps: 225051\n",
      "\t\tRAM: 82.9% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 450 | Score: 210.0 | Avg: 170.35 | Eps: 0.815 | LR: 0.000100 | Steps: 230451\n",
      "Episode 460 | Score: 135.0 | Avg: 164.55 | Eps: 0.811 | LR: 0.000100 | Steps: 235782\n",
      "Episode 470 | Score: 120.0 | Avg: 167.30 | Eps: 0.807 | LR: 0.000100 | Steps: 240901\n",
      "Episode 480 | Score: 215.0 | Avg: 170.00 | Eps: 0.804 | LR: 0.000100 | Steps: 246062\n",
      "\t\tRAM: 83.1% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 490 | Score: 155.0 | Avg: 172.40 | Eps: 0.800 | LR: 0.000100 | Steps: 250690\n",
      "Episode 500 | Score: 215.0 | Avg: 171.75 | Eps: 0.797 | LR: 0.000100 | Steps: 256120\n",
      "Episode 510 | Score: 155.0 | Avg: 176.50 | Eps: 0.793 | LR: 0.000100 | Steps: 261953\n",
      "Episode 520 | Score: 180.0 | Avg: 176.05 | Eps: 0.789 | LR: 0.000100 | Steps: 267658\n",
      "\t\tRAM: 83.2% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 530 | Score: 120.0 | Avg: 178.45 | Eps: 0.785 | LR: 0.000100 | Steps: 272773\n",
      "Episode 540 | Score: 110.0 | Avg: 177.10 | Eps: 0.781 | LR: 0.000100 | Steps: 278122\n",
      "Episode 550 | Score: 120.0 | Avg: 173.40 | Eps: 0.778 | LR: 0.000100 | Steps: 283279\n",
      "Episode 560 | Score: 120.0 | Avg: 179.20 | Eps: 0.774 | LR: 0.000100 | Steps: 289311\n",
      "\t\tRAM: 83.3% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 570 | Score: 395.0 | Avg: 185.50 | Eps: 0.770 | LR: 0.000100 | Steps: 295257\n",
      "Episode 580 | Score: 135.0 | Avg: 190.65 | Eps: 0.766 | LR: 0.000100 | Steps: 301474\n",
      "Episode 590 | Score: 255.0 | Avg: 190.10 | Eps: 0.762 | LR: 0.000100 | Steps: 306648\n",
      "Episode 600 | Score: 180.0 | Avg: 191.90 | Eps: 0.759 | LR: 0.000100 | Steps: 312095\n",
      "\t\tRAM: 83.1% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 610 | Score: 110.0 | Avg: 191.00 | Eps: 0.755 | LR: 0.000100 | Steps: 317648\n",
      "Episode 620 | Score: 180.0 | Avg: 194.35 | Eps: 0.751 | LR: 0.000100 | Steps: 323667\n",
      "Episode 630 | Score: 215.0 | Avg: 189.35 | Eps: 0.748 | LR: 0.000100 | Steps: 328434\n",
      "Episode 640 | Score: 80.0 | Avg: 190.90 | Eps: 0.745 | LR: 0.000100 | Steps: 333538\n",
      "\t\tRAM: 83.4% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 650 | Score: 110.0 | Avg: 199.55 | Eps: 0.741 | LR: 0.000100 | Steps: 339057\n",
      "Episode 660 | Score: 210.0 | Avg: 197.30 | Eps: 0.738 | LR: 0.000100 | Steps: 344079\n",
      "Episode 670 | Score: 115.0 | Avg: 188.50 | Eps: 0.735 | LR: 0.000100 | Steps: 348922\n",
      "Episode 680 | Score: 110.0 | Avg: 181.90 | Eps: 0.732 | LR: 0.000100 | Steps: 353260\n",
      "\t\tRAM: 83.3% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 690 | Score: 215.0 | Avg: 188.85 | Eps: 0.728 | LR: 0.000100 | Steps: 359153\n",
      "Episode 700 | Score: 420.0 | Avg: 200.25 | Eps: 0.724 | LR: 0.000100 | Steps: 365745\n",
      "Episode 710 | Score: 180.0 | Avg: 195.85 | Eps: 0.721 | LR: 0.000100 | Steps: 370294\n",
      "Episode 720 | Score: 230.0 | Avg: 198.15 | Eps: 0.718 | LR: 0.000100 | Steps: 376195\n",
      "\t\tRAM: 82.9% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 730 | Score: 45.0 | Avg: 209.15 | Eps: 0.714 | LR: 0.000100 | Steps: 381849\n",
      "Episode 740 | Score: 205.0 | Avg: 213.25 | Eps: 0.710 | LR: 0.000100 | Steps: 388809\n",
      "Episode 750 | Score: 210.0 | Avg: 212.25 | Eps: 0.707 | LR: 0.000100 | Steps: 394089\n",
      "Episode 760 | Score: 170.0 | Avg: 209.80 | Eps: 0.704 | LR: 0.000100 | Steps: 399060\n",
      "\t\tRAM: 83.2% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 770 | Score: 210.0 | Avg: 219.90 | Eps: 0.701 | LR: 0.000100 | Steps: 404584\n",
      "Episode 780 | Score: 155.0 | Avg: 228.30 | Eps: 0.697 | LR: 0.000100 | Steps: 410132\n",
      "Episode 790 | Score: 180.0 | Avg: 222.80 | Eps: 0.694 | LR: 0.000100 | Steps: 414831\n",
      "Episode 800 | Score: 260.0 | Avg: 218.15 | Eps: 0.691 | LR: 0.000100 | Steps: 420644\n",
      "\t\tNew best avg: 218.15 - saved to DoubleDQN_PER_20251202_044249_best.pth\n",
      "\t\tRAM: 82.9% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 810 | Score: 150.0 | Avg: 218.40 | Eps: 0.689 | LR: 0.000100 | Steps: 424661\n",
      "Episode 820 | Score: 210.0 | Avg: 217.75 | Eps: 0.685 | LR: 0.000100 | Steps: 430866\n",
      "Episode 830 | Score: 85.0 | Avg: 215.50 | Eps: 0.682 | LR: 0.000100 | Steps: 436318\n",
      "Episode 840 | Score: 215.0 | Avg: 210.85 | Eps: 0.679 | LR: 0.000100 | Steps: 441502\n",
      "\t\tRAM: 82.8% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 850 | Score: 265.0 | Avg: 206.90 | Eps: 0.676 | LR: 0.000100 | Steps: 446803\n",
      "Episode 860 | Score: 180.0 | Avg: 218.90 | Eps: 0.672 | LR: 0.000100 | Steps: 453400\n",
      "Episode 870 | Score: 350.0 | Avg: 218.30 | Eps: 0.668 | LR: 0.000100 | Steps: 459445\n",
      "Episode 880 | Score: 825.0 | Avg: 227.35 | Eps: 0.665 | LR: 0.000100 | Steps: 466348\n",
      "\t\tRAM: 83.1% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 890 | Score: 360.0 | Avg: 241.95 | Eps: 0.661 | LR: 0.000100 | Steps: 472986\n",
      "Episode 900 | Score: 240.0 | Avg: 241.05 | Eps: 0.658 | LR: 0.000100 | Steps: 478555\n",
      "Episode 910 | Score: 160.0 | Avg: 246.50 | Eps: 0.655 | LR: 0.000100 | Steps: 483866\n",
      "Episode 920 | Score: 150.0 | Avg: 246.35 | Eps: 0.652 | LR: 0.000100 | Steps: 489311\n",
      "\t\tRAM: 82.8% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 930 | Score: 285.0 | Avg: 251.70 | Eps: 0.649 | LR: 0.000100 | Steps: 495060\n",
      "Episode 940 | Score: 290.0 | Avg: 255.20 | Eps: 0.646 | LR: 0.000100 | Steps: 500594\n",
      "Episode 950 | Score: 180.0 | Avg: 258.20 | Eps: 0.643 | LR: 0.000100 | Steps: 506120\n",
      "Episode 960 | Score: 280.0 | Avg: 252.10 | Eps: 0.639 | LR: 0.000100 | Steps: 512315\n",
      "\t\tRAM: 83.3% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 970 | Score: 90.0 | Avg: 252.90 | Eps: 0.636 | LR: 0.000100 | Steps: 518037\n",
      "Episode 980 | Score: 395.0 | Avg: 241.30 | Eps: 0.633 | LR: 0.000100 | Steps: 524612\n",
      "Episode 990 | Score: 125.0 | Avg: 228.90 | Eps: 0.630 | LR: 0.000100 | Steps: 530232\n",
      "Episode 1000 | Score: 230.0 | Avg: 227.15 | Eps: 0.627 | LR: 0.000100 | Steps: 535323\n",
      "\t\tRAM: 83.2% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 1010 | Score: 135.0 | Avg: 226.65 | Eps: 0.624 | LR: 0.000100 | Steps: 540432\n",
      "Episode 1020 | Score: 235.0 | Avg: 231.60 | Eps: 0.621 | LR: 0.000100 | Steps: 547128\n",
      "Episode 1030 | Score: 210.0 | Avg: 222.80 | Eps: 0.619 | LR: 0.000100 | Steps: 551433\n",
      "Episode 1040 | Score: 445.0 | Avg: 234.30 | Eps: 0.614 | LR: 0.000100 | Steps: 559208\n",
      "\t\tRAM: 83.2% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 1050 | Score: 195.0 | Avg: 229.35 | Eps: 0.612 | LR: 0.000100 | Steps: 564217\n",
      "Episode 1060 | Score: 155.0 | Avg: 227.75 | Eps: 0.609 | LR: 0.000100 | Steps: 569514\n",
      "Episode 1070 | Score: 210.0 | Avg: 227.00 | Eps: 0.606 | LR: 0.000100 | Steps: 574972\n",
      "Episode 1080 | Score: 240.0 | Avg: 230.20 | Eps: 0.604 | LR: 0.000100 | Steps: 580365\n",
      "\t\tRAM: 83.2% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 1090 | Score: 230.0 | Avg: 233.30 | Eps: 0.601 | LR: 0.000100 | Steps: 586027\n",
      "Episode 1100 | Score: 405.0 | Avg: 232.90 | Eps: 0.598 | LR: 0.000100 | Steps: 591261\n",
      "Episode 1110 | Score: 185.0 | Avg: 231.45 | Eps: 0.596 | LR: 0.000100 | Steps: 596594\n",
      "Episode 1120 | Score: 125.0 | Avg: 225.45 | Eps: 0.593 | LR: 0.000100 | Steps: 601614\n",
      "\t\tRAM: 83.3% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 1130 | Score: 275.0 | Avg: 234.50 | Eps: 0.590 | LR: 0.000100 | Steps: 607623\n",
      "Episode 1140 | Score: 435.0 | Avg: 222.70 | Eps: 0.588 | LR: 0.000100 | Steps: 612957\n",
      "Episode 1150 | Score: 560.0 | Avg: 228.15 | Eps: 0.585 | LR: 0.000100 | Steps: 617883\n",
      "Episode 1160 | Score: 320.0 | Avg: 233.40 | Eps: 0.582 | LR: 0.000100 | Steps: 624484\n",
      "\t\tRAM: 83.0% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 1170 | Score: 170.0 | Avg: 232.25 | Eps: 0.579 | LR: 0.000100 | Steps: 630046\n",
      "Episode 1180 | Score: 65.0 | Avg: 226.20 | Eps: 0.577 | LR: 0.000100 | Steps: 635501\n",
      "Episode 1190 | Score: 225.0 | Avg: 223.80 | Eps: 0.574 | LR: 0.000100 | Steps: 640593\n",
      "Episode 1200 | Score: 225.0 | Avg: 223.80 | Eps: 0.572 | LR: 0.000100 | Steps: 646117\n",
      "\t\tNew best avg: 223.80 - saved to DoubleDQN_PER_20251202_062942_best.pth\n",
      "\t\tRAM: 83.1% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 1210 | Score: 475.0 | Avg: 232.00 | Eps: 0.569 | LR: 0.000100 | Steps: 652090\n",
      "Episode 1220 | Score: 210.0 | Avg: 232.00 | Eps: 0.567 | LR: 0.000100 | Steps: 657022\n",
      "Episode 1230 | Score: 530.0 | Avg: 229.40 | Eps: 0.564 | LR: 0.000100 | Steps: 662457\n",
      "Episode 1240 | Score: 290.0 | Avg: 226.85 | Eps: 0.562 | LR: 0.000100 | Steps: 667270\n",
      "\t\tRAM: 83.1% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 1250 | Score: 250.0 | Avg: 226.85 | Eps: 0.559 | LR: 0.000100 | Steps: 673596\n",
      "Episode 1260 | Score: 210.0 | Avg: 220.95 | Eps: 0.556 | LR: 0.000100 | Steps: 679336\n",
      "Episode 1270 | Score: 160.0 | Avg: 221.40 | Eps: 0.554 | LR: 0.000100 | Steps: 684886\n",
      "Episode 1280 | Score: 215.0 | Avg: 226.05 | Eps: 0.551 | LR: 0.000100 | Steps: 690981\n",
      "\t\tRAM: 83.1% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 1290 | Score: 150.0 | Avg: 233.90 | Eps: 0.548 | LR: 0.000100 | Steps: 697724\n",
      "Episode 1300 | Score: 215.0 | Avg: 239.55 | Eps: 0.545 | LR: 0.000100 | Steps: 703503\n",
      "Episode 1310 | Score: 210.0 | Avg: 226.30 | Eps: 0.543 | LR: 0.000100 | Steps: 707774\n",
      "Episode 1320 | Score: 160.0 | Avg: 228.25 | Eps: 0.541 | LR: 0.000100 | Steps: 713371\n",
      "\t\tRAM: 81.3% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 1330 | Score: 80.0 | Avg: 226.65 | Eps: 0.539 | LR: 0.000100 | Steps: 718610\n",
      "Episode 1340 | Score: 180.0 | Avg: 232.80 | Eps: 0.536 | LR: 0.000100 | Steps: 724428\n",
      "Episode 1350 | Score: 80.0 | Avg: 227.10 | Eps: 0.534 | LR: 0.000100 | Steps: 729231\n",
      "Episode 1360 | Score: 105.0 | Avg: 232.60 | Eps: 0.531 | LR: 0.000100 | Steps: 735219\n",
      "\t\tRAM: 83.2% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 1370 | Score: 215.0 | Avg: 234.40 | Eps: 0.529 | LR: 0.000100 | Steps: 740090\n",
      "Episode 1380 | Score: 215.0 | Avg: 233.05 | Eps: 0.527 | LR: 0.000100 | Steps: 745556\n",
      "Episode 1390 | Score: 210.0 | Avg: 223.30 | Eps: 0.525 | LR: 0.000100 | Steps: 750306\n",
      "Episode 1400 | Score: 415.0 | Avg: 223.40 | Eps: 0.522 | LR: 0.000100 | Steps: 756440\n",
      "\t\tRAM: 82.1% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 1410 | Score: 135.0 | Avg: 231.10 | Eps: 0.520 | LR: 0.000100 | Steps: 761269\n",
      "Episode 1420 | Score: 215.0 | Avg: 227.25 | Eps: 0.518 | LR: 0.000100 | Steps: 766207\n",
      "Episode 1430 | Score: 225.0 | Avg: 241.40 | Eps: 0.515 | LR: 0.000100 | Steps: 773320\n",
      "Episode 1440 | Score: 210.0 | Avg: 234.05 | Eps: 0.513 | LR: 0.000100 | Steps: 778160\n",
      "\t\tRAM: 83.2% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 1450 | Score: 240.0 | Avg: 240.50 | Eps: 0.511 | LR: 0.000100 | Steps: 784362\n",
      "Episode 1460 | Score: 180.0 | Avg: 233.60 | Eps: 0.509 | LR: 0.000100 | Steps: 788792\n",
      "Episode 1470 | Score: 215.0 | Avg: 235.85 | Eps: 0.507 | LR: 0.000100 | Steps: 794349\n",
      "Episode 1480 | Score: 260.0 | Avg: 240.30 | Eps: 0.505 | LR: 0.000100 | Steps: 799662\n",
      "\t\tRAM: 83.0% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 1490 | Score: 180.0 | Avg: 246.10 | Eps: 0.502 | LR: 0.000100 | Steps: 805217\n",
      "Episode 1500 | Score: 190.0 | Avg: 248.40 | Eps: 0.500 | LR: 0.000100 | Steps: 811806\n",
      "Episode 1510 | Score: 175.0 | Avg: 251.25 | Eps: 0.497 | LR: 0.000100 | Steps: 817284\n",
      "Episode 1520 | Score: 115.0 | Avg: 251.45 | Eps: 0.495 | LR: 0.000100 | Steps: 822511\n",
      "\t\tRAM: 83.5% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 1530 | Score: 360.0 | Avg: 238.05 | Eps: 0.493 | LR: 0.000100 | Steps: 828005\n",
      "Episode 1540 | Score: 180.0 | Avg: 242.50 | Eps: 0.491 | LR: 0.000100 | Steps: 833560\n",
      "Episode 1550 | Score: 170.0 | Avg: 244.05 | Eps: 0.489 | LR: 0.000100 | Steps: 839686\n",
      "Episode 1560 | Score: 80.0 | Avg: 254.70 | Eps: 0.486 | LR: 0.000100 | Steps: 845972\n",
      "\t\tRAM: 82.4% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 1570 | Score: 350.0 | Avg: 254.55 | Eps: 0.484 | LR: 0.000100 | Steps: 851360\n",
      "Episode 1580 | Score: 210.0 | Avg: 251.40 | Eps: 0.482 | LR: 0.000100 | Steps: 857162\n",
      "Episode 1590 | Score: 180.0 | Avg: 251.45 | Eps: 0.480 | LR: 0.000100 | Steps: 862818\n",
      "Episode 1600 | Score: 105.0 | Avg: 251.95 | Eps: 0.477 | LR: 0.000100 | Steps: 869320\n",
      "\t\tNew best avg: 251.95 - saved to DoubleDQN_PER_20251202_081620_best.pth\n",
      "\t\tRAM: 82.7% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 1610 | Score: 290.0 | Avg: 260.85 | Eps: 0.475 | LR: 0.000100 | Steps: 875669\n",
      "Episode 1620 | Score: 460.0 | Avg: 279.50 | Eps: 0.472 | LR: 0.000100 | Steps: 883072\n",
      "Episode 1630 | Score: 750.0 | Avg: 288.10 | Eps: 0.470 | LR: 0.000100 | Steps: 889322\n",
      "Episode 1640 | Score: 210.0 | Avg: 298.50 | Eps: 0.467 | LR: 0.000100 | Steps: 896087\n",
      "\t\tRAM: 83.2% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 1650 | Score: 210.0 | Avg: 307.80 | Eps: 0.465 | LR: 0.000100 | Steps: 902584\n",
      "Episode 1660 | Score: 315.0 | Avg: 306.00 | Eps: 0.463 | LR: 0.000100 | Steps: 908980\n",
      "Episode 1670 | Score: 370.0 | Avg: 309.90 | Eps: 0.460 | LR: 0.000100 | Steps: 915535\n",
      "Episode 1680 | Score: 345.0 | Avg: 314.15 | Eps: 0.458 | LR: 0.000100 | Steps: 921985\n",
      "\t\tRAM: 83.3% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 1690 | Score: 315.0 | Avg: 316.30 | Eps: 0.456 | LR: 0.000100 | Steps: 927894\n",
      "Episode 1700 | Score: 125.0 | Avg: 307.25 | Eps: 0.454 | LR: 0.000100 | Steps: 933580\n",
      "Episode 1710 | Score: 505.0 | Avg: 314.70 | Eps: 0.451 | LR: 0.000100 | Steps: 940243\n",
      "Episode 1720 | Score: 255.0 | Avg: 296.05 | Eps: 0.450 | LR: 0.000100 | Steps: 945338\n",
      "\t\tRAM: 83.1% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 1730 | Score: 245.0 | Avg: 284.25 | Eps: 0.448 | LR: 0.000100 | Steps: 950252\n",
      "Episode 1740 | Score: 180.0 | Avg: 281.00 | Eps: 0.446 | LR: 0.000100 | Steps: 956541\n",
      "Episode 1750 | Score: 255.0 | Avg: 270.05 | Eps: 0.444 | LR: 0.000100 | Steps: 962463\n",
      "Episode 1760 | Score: 155.0 | Avg: 270.65 | Eps: 0.442 | LR: 0.000100 | Steps: 968161\n",
      "\t\tRAM: 82.6% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 1770 | Score: 300.0 | Avg: 273.70 | Eps: 0.440 | LR: 0.000100 | Steps: 974633\n",
      "Episode 1780 | Score: 90.0 | Avg: 269.60 | Eps: 0.438 | LR: 0.000100 | Steps: 980781\n",
      "Episode 1790 | Score: 225.0 | Avg: 266.05 | Eps: 0.436 | LR: 0.000100 | Steps: 986117\n",
      "Episode 1800 | Score: 350.0 | Avg: 273.95 | Eps: 0.433 | LR: 0.000100 | Steps: 992975\n",
      "\t\tRAM: 82.8% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 1810 | Score: 120.0 | Avg: 255.25 | Eps: 0.432 | LR: 0.000100 | Steps: 998328\n",
      "Episode 1820 | Score: 225.0 | Avg: 259.70 | Eps: 0.430 | LR: 0.000100 | Steps: 1003964\n",
      "Episode 1830 | Score: 210.0 | Avg: 266.95 | Eps: 0.428 | LR: 0.000100 | Steps: 1010167\n",
      "Episode 1840 | Score: 95.0 | Avg: 254.75 | Eps: 0.426 | LR: 0.000100 | Steps: 1014790\n",
      "\t\tRAM: 83.2% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 1850 | Score: 155.0 | Avg: 252.70 | Eps: 0.424 | LR: 0.000100 | Steps: 1020442\n",
      "Episode 1860 | Score: 155.0 | Avg: 254.00 | Eps: 0.422 | LR: 0.000100 | Steps: 1027203\n",
      "Episode 1870 | Score: 210.0 | Avg: 253.35 | Eps: 0.420 | LR: 0.000100 | Steps: 1033580\n",
      "Episode 1880 | Score: 315.0 | Avg: 257.45 | Eps: 0.418 | LR: 0.000100 | Steps: 1040173\n",
      "\t\tRAM: 83.5% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 1890 | Score: 315.0 | Avg: 256.05 | Eps: 0.416 | LR: 0.000100 | Steps: 1046015\n",
      "Episode 1900 | Score: 195.0 | Avg: 262.10 | Eps: 0.414 | LR: 0.000100 | Steps: 1052569\n",
      "Episode 1910 | Score: 705.0 | Avg: 268.40 | Eps: 0.412 | LR: 0.000100 | Steps: 1058160\n",
      "Episode 1920 | Score: 235.0 | Avg: 268.30 | Eps: 0.411 | LR: 0.000100 | Steps: 1063476\n",
      "\t\tRAM: 83.3% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 1930 | Score: 315.0 | Avg: 270.35 | Eps: 0.409 | LR: 0.000100 | Steps: 1070433\n",
      "Episode 1940 | Score: 365.0 | Avg: 290.45 | Eps: 0.406 | LR: 0.000100 | Steps: 1078308\n",
      "Episode 1950 | Score: 320.0 | Avg: 293.40 | Eps: 0.404 | LR: 0.000100 | Steps: 1084292\n",
      "Episode 1960 | Score: 500.0 | Avg: 290.60 | Eps: 0.403 | LR: 0.000100 | Steps: 1090293\n",
      "\t\tRAM: 83.5% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 1970 | Score: 230.0 | Avg: 281.90 | Eps: 0.401 | LR: 0.000100 | Steps: 1096616\n",
      "Episode 1980 | Score: 335.0 | Avg: 283.80 | Eps: 0.399 | LR: 0.000100 | Steps: 1102617\n",
      "Episode 1990 | Score: 285.0 | Avg: 285.25 | Eps: 0.397 | LR: 0.000100 | Steps: 1107840\n",
      "Episode 2000 | Score: 165.0 | Avg: 268.10 | Eps: 0.396 | LR: 0.000100 | Steps: 1113014\n",
      "\t\tNew best avg: 268.10 - saved to DoubleDQN_PER_20251202_101312_best.pth\n",
      "\t\tRAM: 83.0% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 2010 | Score: 365.0 | Avg: 267.70 | Eps: 0.394 | LR: 0.000100 | Steps: 1119721\n",
      "Episode 2020 | Score: 515.0 | Avg: 269.80 | Eps: 0.392 | LR: 0.000100 | Steps: 1125965\n",
      "Episode 2030 | Score: 185.0 | Avg: 267.45 | Eps: 0.390 | LR: 0.000100 | Steps: 1131647\n",
      "Episode 2040 | Score: 270.0 | Avg: 256.30 | Eps: 0.388 | LR: 0.000100 | Steps: 1138276\n",
      "\t\tRAM: 83.4% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 2050 | Score: 80.0 | Avg: 252.15 | Eps: 0.387 | LR: 0.000100 | Steps: 1143878\n",
      "Episode 2060 | Score: 400.0 | Avg: 261.05 | Eps: 0.385 | LR: 0.000100 | Steps: 1151401\n",
      "Episode 2070 | Score: 275.0 | Avg: 263.30 | Eps: 0.383 | LR: 0.000100 | Steps: 1157013\n",
      "Episode 2080 | Score: 345.0 | Avg: 263.90 | Eps: 0.381 | LR: 0.000100 | Steps: 1163474\n",
      "\t\tRAM: 83.3% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 2090 | Score: 545.0 | Avg: 269.25 | Eps: 0.379 | LR: 0.000100 | Steps: 1169887\n",
      "Episode 2100 | Score: 355.0 | Avg: 281.10 | Eps: 0.378 | LR: 0.000100 | Steps: 1176274\n",
      "Episode 2110 | Score: 210.0 | Avg: 275.80 | Eps: 0.376 | LR: 0.000100 | Steps: 1181956\n",
      "Episode 2120 | Score: 275.0 | Avg: 278.90 | Eps: 0.374 | LR: 0.000100 | Steps: 1189042\n",
      "\t\tRAM: 82.8% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 2130 | Score: 240.0 | Avg: 282.40 | Eps: 0.372 | LR: 0.000100 | Steps: 1194950\n",
      "Episode 2140 | Score: 210.0 | Avg: 284.15 | Eps: 0.371 | LR: 0.000100 | Steps: 1201256\n",
      "Episode 2150 | Score: 260.0 | Avg: 285.15 | Eps: 0.369 | LR: 0.000100 | Steps: 1206449\n",
      "Episode 2160 | Score: 340.0 | Avg: 278.55 | Eps: 0.368 | LR: 0.000100 | Steps: 1212412\n",
      "\t\tRAM: 83.3% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 2170 | Score: 265.0 | Avg: 277.70 | Eps: 0.366 | LR: 0.000100 | Steps: 1218813\n",
      "Episode 2180 | Score: 465.0 | Avg: 274.60 | Eps: 0.364 | LR: 0.000100 | Steps: 1224949\n",
      "Episode 2190 | Score: 275.0 | Avg: 273.20 | Eps: 0.363 | LR: 0.000100 | Steps: 1231218\n",
      "Episode 2200 | Score: 230.0 | Avg: 271.50 | Eps: 0.361 | LR: 0.000100 | Steps: 1237575\n",
      "\t\tRAM: 83.0% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 2210 | Score: 155.0 | Avg: 274.70 | Eps: 0.359 | LR: 0.000100 | Steps: 1243849\n",
      "Episode 2220 | Score: 470.0 | Avg: 272.95 | Eps: 0.358 | LR: 0.000100 | Steps: 1250132\n",
      "Episode 2230 | Score: 415.0 | Avg: 269.70 | Eps: 0.356 | LR: 0.000100 | Steps: 1256209\n",
      "Episode 2240 | Score: 590.0 | Avg: 270.70 | Eps: 0.355 | LR: 0.000100 | Steps: 1262296\n",
      "\t\tRAM: 83.1% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 2250 | Score: 215.0 | Avg: 275.50 | Eps: 0.353 | LR: 0.000100 | Steps: 1268633\n",
      "Episode 2260 | Score: 180.0 | Avg: 270.20 | Eps: 0.352 | LR: 0.000100 | Steps: 1274044\n",
      "Episode 2270 | Score: 135.0 | Avg: 270.20 | Eps: 0.350 | LR: 0.000100 | Steps: 1279902\n",
      "Episode 2280 | Score: 210.0 | Avg: 267.30 | Eps: 0.349 | LR: 0.000100 | Steps: 1285509\n",
      "\t\tRAM: 83.2% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 2290 | Score: 300.0 | Avg: 269.25 | Eps: 0.347 | LR: 0.000100 | Steps: 1292007\n",
      "Episode 2300 | Score: 210.0 | Avg: 262.35 | Eps: 0.346 | LR: 0.000100 | Steps: 1297728\n",
      "Episode 2310 | Score: 215.0 | Avg: 268.10 | Eps: 0.344 | LR: 0.000100 | Steps: 1304659\n",
      "Episode 2320 | Score: 470.0 | Avg: 264.45 | Eps: 0.343 | LR: 0.000100 | Steps: 1310037\n",
      "\t\tRAM: 83.1% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 2330 | Score: 225.0 | Avg: 264.95 | Eps: 0.341 | LR: 0.000100 | Steps: 1315998\n",
      "Episode 2340 | Score: 260.0 | Avg: 260.05 | Eps: 0.340 | LR: 0.000100 | Steps: 1321656\n",
      "Episode 2350 | Score: 450.0 | Avg: 261.40 | Eps: 0.338 | LR: 0.000100 | Steps: 1328677\n",
      "Episode 2360 | Score: 180.0 | Avg: 263.05 | Eps: 0.337 | LR: 0.000100 | Steps: 1334386\n",
      "\t\tRAM: 82.7% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 2370 | Score: 155.0 | Avg: 263.15 | Eps: 0.336 | LR: 0.000100 | Steps: 1339756\n",
      "Episode 2380 | Score: 205.0 | Avg: 268.50 | Eps: 0.334 | LR: 0.000100 | Steps: 1346009\n",
      "Episode 2390 | Score: 165.0 | Avg: 265.45 | Eps: 0.333 | LR: 0.000100 | Steps: 1352492\n",
      "Episode 2400 | Score: 510.0 | Avg: 277.05 | Eps: 0.331 | LR: 0.000100 | Steps: 1359616\n",
      "\t\tNew best avg: 277.05 - saved to DoubleDQN_PER_20251202_121156_best.pth\n",
      "\t\tRAM: 83.1% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 2410 | Score: 120.0 | Avg: 266.60 | Eps: 0.330 | LR: 0.000100 | Steps: 1365853\n",
      "Episode 2420 | Score: 365.0 | Avg: 268.10 | Eps: 0.328 | LR: 0.000100 | Steps: 1371752\n",
      "Episode 2430 | Score: 210.0 | Avg: 267.70 | Eps: 0.327 | LR: 0.000100 | Steps: 1378047\n",
      "Episode 2440 | Score: 230.0 | Avg: 269.35 | Eps: 0.325 | LR: 0.000100 | Steps: 1384633\n",
      "\t\tRAM: 83.2% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 2450 | Score: 410.0 | Avg: 259.80 | Eps: 0.324 | LR: 0.000100 | Steps: 1389898\n",
      "Episode 2460 | Score: 315.0 | Avg: 266.90 | Eps: 0.323 | LR: 0.000100 | Steps: 1397167\n",
      "Episode 2470 | Score: 210.0 | Avg: 266.65 | Eps: 0.321 | LR: 0.000100 | Steps: 1403243\n",
      "Episode 2480 | Score: 310.0 | Avg: 262.65 | Eps: 0.320 | LR: 0.000100 | Steps: 1409856\n",
      "\t\tRAM: 83.3% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 2490 | Score: 530.0 | Avg: 268.95 | Eps: 0.318 | LR: 0.000100 | Steps: 1416758\n",
      "Episode 2500 | Score: 375.0 | Avg: 266.15 | Eps: 0.317 | LR: 0.000100 | Steps: 1423742\n",
      "Episode 2510 | Score: 480.0 | Avg: 276.00 | Eps: 0.315 | LR: 0.000100 | Steps: 1430993\n",
      "Episode 2520 | Score: 275.0 | Avg: 279.15 | Eps: 0.314 | LR: 0.000100 | Steps: 1438056\n",
      "\t\tRAM: 82.6% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 2530 | Score: 255.0 | Avg: 281.65 | Eps: 0.312 | LR: 0.000100 | Steps: 1445279\n",
      "Episode 2540 | Score: 215.0 | Avg: 289.15 | Eps: 0.311 | LR: 0.000100 | Steps: 1452785\n",
      "Episode 2550 | Score: 270.0 | Avg: 307.45 | Eps: 0.309 | LR: 0.000100 | Steps: 1460603\n",
      "Episode 2560 | Score: 535.0 | Avg: 306.75 | Eps: 0.307 | LR: 0.000100 | Steps: 1468208\n",
      "\t\tRAM: 83.0% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 2570 | Score: 210.0 | Avg: 319.55 | Eps: 0.306 | LR: 0.000100 | Steps: 1475759\n",
      "Episode 2580 | Score: 240.0 | Avg: 318.80 | Eps: 0.305 | LR: 0.000100 | Steps: 1481705\n",
      "Episode 2590 | Score: 475.0 | Avg: 325.85 | Eps: 0.303 | LR: 0.000100 | Steps: 1490219\n",
      "Episode 2600 | Score: 370.0 | Avg: 324.25 | Eps: 0.301 | LR: 0.000100 | Steps: 1497305\n",
      "\t\tRAM: 83.2% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 2610 | Score: 290.0 | Avg: 319.15 | Eps: 0.300 | LR: 0.000100 | Steps: 1504179\n",
      "Episode 2620 | Score: 350.0 | Avg: 322.45 | Eps: 0.298 | LR: 0.000100 | Steps: 1512506\n",
      "Episode 2630 | Score: 260.0 | Avg: 323.30 | Eps: 0.297 | LR: 0.000100 | Steps: 1520308\n",
      "Episode 2640 | Score: 230.0 | Avg: 314.45 | Eps: 0.296 | LR: 0.000100 | Steps: 1526004\n",
      "\t\tRAM: 83.4% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 2650 | Score: 235.0 | Avg: 303.60 | Eps: 0.294 | LR: 0.000100 | Steps: 1532612\n",
      "Episode 2660 | Score: 210.0 | Avg: 305.80 | Eps: 0.293 | LR: 0.000100 | Steps: 1540526\n",
      "Episode 2670 | Score: 415.0 | Avg: 305.25 | Eps: 0.291 | LR: 0.000100 | Steps: 1548424\n",
      "Episode 2680 | Score: 210.0 | Avg: 307.20 | Eps: 0.290 | LR: 0.000100 | Steps: 1555251\n",
      "\t\tRAM: 82.8% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 2690 | Score: 545.0 | Avg: 300.85 | Eps: 0.289 | LR: 0.000100 | Steps: 1562490\n",
      "Episode 2700 | Score: 225.0 | Avg: 306.10 | Eps: 0.287 | LR: 0.000100 | Steps: 1569290\n",
      "Episode 2710 | Score: 240.0 | Avg: 303.60 | Eps: 0.286 | LR: 0.000100 | Steps: 1575800\n",
      "Episode 2720 | Score: 550.0 | Avg: 310.45 | Eps: 0.285 | LR: 0.000100 | Steps: 1583485\n",
      "\t\tRAM: 83.4% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 2730 | Score: 275.0 | Avg: 314.25 | Eps: 0.283 | LR: 0.000100 | Steps: 1591542\n",
      "Episode 2740 | Score: 490.0 | Avg: 317.65 | Eps: 0.282 | LR: 0.000100 | Steps: 1597892\n",
      "Episode 2750 | Score: 180.0 | Avg: 325.40 | Eps: 0.281 | LR: 0.000100 | Steps: 1605371\n",
      "Episode 2760 | Score: 470.0 | Avg: 324.35 | Eps: 0.280 | LR: 0.000100 | Steps: 1612047\n",
      "\t\tRAM: 82.7% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 2770 | Score: 105.0 | Avg: 315.60 | Eps: 0.278 | LR: 0.000100 | Steps: 1618781\n",
      "Episode 2780 | Score: 310.0 | Avg: 317.20 | Eps: 0.277 | LR: 0.000100 | Steps: 1625557\n",
      "Episode 2790 | Score: 375.0 | Avg: 322.55 | Eps: 0.276 | LR: 0.000100 | Steps: 1634089\n",
      "Episode 2800 | Score: 180.0 | Avg: 317.30 | Eps: 0.274 | LR: 0.000100 | Steps: 1641342\n",
      "\t\tNew best avg: 317.30 - saved to DoubleDQN_PER_20251202_142749_best.pth\n",
      "\t\tRAM: 83.1% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 2810 | Score: 125.0 | Avg: 322.75 | Eps: 0.273 | LR: 0.000100 | Steps: 1648173\n",
      "Episode 2820 | Score: 600.0 | Avg: 317.30 | Eps: 0.272 | LR: 0.000100 | Steps: 1655407\n",
      "Episode 2830 | Score: 695.0 | Avg: 326.25 | Eps: 0.271 | LR: 0.000100 | Steps: 1662667\n",
      "Episode 2840 | Score: 210.0 | Avg: 333.60 | Eps: 0.269 | LR: 0.000100 | Steps: 1670178\n",
      "\t\tRAM: 83.4% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 2850 | Score: 260.0 | Avg: 324.50 | Eps: 0.268 | LR: 0.000100 | Steps: 1676820\n",
      "Episode 2860 | Score: 210.0 | Avg: 320.70 | Eps: 0.267 | LR: 0.000100 | Steps: 1682970\n",
      "Episode 2870 | Score: 285.0 | Avg: 310.95 | Eps: 0.266 | LR: 0.000100 | Steps: 1688191\n",
      "Episode 2880 | Score: 335.0 | Avg: 312.10 | Eps: 0.265 | LR: 0.000100 | Steps: 1694845\n",
      "\t\tRAM: 83.3% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 2890 | Score: 280.0 | Avg: 305.65 | Eps: 0.264 | LR: 0.000100 | Steps: 1701207\n",
      "Episode 2900 | Score: 365.0 | Avg: 310.80 | Eps: 0.263 | LR: 0.000100 | Steps: 1708252\n",
      "Episode 2910 | Score: 510.0 | Avg: 316.50 | Eps: 0.262 | LR: 0.000100 | Steps: 1714791\n",
      "Episode 2920 | Score: 305.0 | Avg: 313.15 | Eps: 0.261 | LR: 0.000100 | Steps: 1721844\n",
      "\t\tRAM: 83.4% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 2930 | Score: 105.0 | Avg: 301.85 | Eps: 0.260 | LR: 0.000100 | Steps: 1728144\n",
      "Episode 2940 | Score: 255.0 | Avg: 296.25 | Eps: 0.259 | LR: 0.000100 | Steps: 1734247\n",
      "Episode 2950 | Score: 380.0 | Avg: 300.40 | Eps: 0.258 | LR: 0.000100 | Steps: 1740792\n",
      "Episode 2960 | Score: 410.0 | Avg: 305.65 | Eps: 0.257 | LR: 0.000100 | Steps: 1748495\n",
      "\t\tRAM: 83.3% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 2970 | Score: 325.0 | Avg: 323.25 | Eps: 0.256 | LR: 0.000100 | Steps: 1755216\n",
      "Episode 2980 | Score: 640.0 | Avg: 325.80 | Eps: 0.254 | LR: 0.000100 | Steps: 1762400\n",
      "Episode 2990 | Score: 420.0 | Avg: 335.85 | Eps: 0.253 | LR: 0.000100 | Steps: 1769365\n",
      "Episode 3000 | Score: 210.0 | Avg: 332.30 | Eps: 0.252 | LR: 0.000100 | Steps: 1775978\n",
      "\t\tRAM: 83.2% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 3010 | Score: 185.0 | Avg: 328.40 | Eps: 0.251 | LR: 0.000100 | Steps: 1783008\n",
      "Episode 3020 | Score: 210.0 | Avg: 331.60 | Eps: 0.250 | LR: 0.000100 | Steps: 1790190\n",
      "Episode 3030 | Score: 100.0 | Avg: 328.75 | Eps: 0.249 | LR: 0.000100 | Steps: 1796629\n",
      "Episode 3040 | Score: 295.0 | Avg: 328.20 | Eps: 0.248 | LR: 0.000100 | Steps: 1803510\n",
      "\t\tRAM: 82.9% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 3050 | Score: 190.0 | Avg: 334.05 | Eps: 0.247 | LR: 0.000100 | Steps: 1810530\n",
      "Episode 3060 | Score: 635.0 | Avg: 335.10 | Eps: 0.246 | LR: 0.000100 | Steps: 1817626\n",
      "Episode 3070 | Score: 165.0 | Avg: 335.50 | Eps: 0.245 | LR: 0.000100 | Steps: 1824562\n",
      "Episode 3080 | Score: 80.0 | Avg: 337.00 | Eps: 0.244 | LR: 0.000100 | Steps: 1831410\n",
      "\t\tRAM: 83.2% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 3090 | Score: 210.0 | Avg: 332.90 | Eps: 0.243 | LR: 0.000100 | Steps: 1839539\n",
      "Episode 3100 | Score: 260.0 | Avg: 334.50 | Eps: 0.242 | LR: 0.000100 | Steps: 1846587\n",
      "Episode 3110 | Score: 600.0 | Avg: 337.50 | Eps: 0.241 | LR: 0.000100 | Steps: 1854160\n",
      "Episode 3120 | Score: 315.0 | Avg: 334.35 | Eps: 0.240 | LR: 0.000100 | Steps: 1861001\n",
      "\t\tRAM: 82.8% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 3130 | Score: 615.0 | Avg: 340.10 | Eps: 0.239 | LR: 0.000100 | Steps: 1867993\n",
      "Episode 3140 | Score: 220.0 | Avg: 345.60 | Eps: 0.238 | LR: 0.000100 | Steps: 1875951\n",
      "Episode 3150 | Score: 430.0 | Avg: 347.60 | Eps: 0.237 | LR: 0.000100 | Steps: 1883350\n",
      "Episode 3160 | Score: 210.0 | Avg: 348.25 | Eps: 0.236 | LR: 0.000100 | Steps: 1891307\n",
      "\t\tRAM: 83.2% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 3170 | Score: 220.0 | Avg: 346.85 | Eps: 0.235 | LR: 0.000100 | Steps: 1898381\n",
      "Episode 3180 | Score: 180.0 | Avg: 345.50 | Eps: 0.234 | LR: 0.000100 | Steps: 1905610\n",
      "Episode 3190 | Score: 440.0 | Avg: 353.40 | Eps: 0.233 | LR: 0.000100 | Steps: 1914058\n",
      "Episode 3200 | Score: 300.0 | Avg: 346.30 | Eps: 0.232 | LR: 0.000100 | Steps: 1920619\n",
      "\t\tNew best avg: 346.30 - saved to DoubleDQN_PER_20251202_164236_best.pth\n",
      "\t\tRAM: 83.1% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 3210 | Score: 290.0 | Avg: 346.80 | Eps: 0.231 | LR: 0.000100 | Steps: 1927764\n",
      "Episode 3220 | Score: 485.0 | Avg: 357.00 | Eps: 0.230 | LR: 0.000100 | Steps: 1935519\n",
      "Episode 3230 | Score: 250.0 | Avg: 358.80 | Eps: 0.229 | LR: 0.000100 | Steps: 1942323\n",
      "Episode 3240 | Score: 485.0 | Avg: 355.00 | Eps: 0.228 | LR: 0.000100 | Steps: 1948856\n",
      "\t\tRAM: 83.1% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 3250 | Score: 180.0 | Avg: 353.40 | Eps: 0.227 | LR: 0.000100 | Steps: 1955160\n",
      "Episode 3260 | Score: 210.0 | Avg: 349.95 | Eps: 0.226 | LR: 0.000100 | Steps: 1962340\n",
      "Episode 3270 | Score: 280.0 | Avg: 345.80 | Eps: 0.226 | LR: 0.000100 | Steps: 1968805\n",
      "Episode 3280 | Score: 210.0 | Avg: 343.00 | Eps: 0.225 | LR: 0.000100 | Steps: 1975732\n",
      "\t\tRAM: 83.4% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 3290 | Score: 265.0 | Avg: 335.80 | Eps: 0.224 | LR: 0.000100 | Steps: 1982696\n",
      "Episode 3300 | Score: 210.0 | Avg: 335.35 | Eps: 0.223 | LR: 0.000100 | Steps: 1988635\n",
      "Episode 3310 | Score: 335.0 | Avg: 333.95 | Eps: 0.222 | LR: 0.000100 | Steps: 1996404\n",
      "Episode 3320 | Score: 210.0 | Avg: 325.60 | Eps: 0.221 | LR: 0.000100 | Steps: 2002961\n",
      "\t\tRAM: 82.0% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 3330 | Score: 260.0 | Avg: 321.30 | Eps: 0.221 | LR: 0.000100 | Steps: 2009615\n",
      "Episode 3340 | Score: 365.0 | Avg: 336.60 | Eps: 0.220 | LR: 0.000100 | Steps: 2016945\n",
      "Episode 3350 | Score: 510.0 | Avg: 333.15 | Eps: 0.219 | LR: 0.000100 | Steps: 2023652\n",
      "Episode 3360 | Score: 225.0 | Avg: 329.60 | Eps: 0.218 | LR: 0.000100 | Steps: 2030368\n",
      "\t\tRAM: 82.8% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 3370 | Score: 425.0 | Avg: 336.65 | Eps: 0.217 | LR: 0.000100 | Steps: 2037423\n",
      "Episode 3380 | Score: 635.0 | Avg: 338.20 | Eps: 0.217 | LR: 0.000100 | Steps: 2044319\n",
      "Episode 3390 | Score: 225.0 | Avg: 334.35 | Eps: 0.216 | LR: 0.000100 | Steps: 2051146\n",
      "Episode 3400 | Score: 555.0 | Avg: 341.50 | Eps: 0.215 | LR: 0.000100 | Steps: 2057385\n",
      "\t\tRAM: 83.1% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 3410 | Score: 410.0 | Avg: 339.95 | Eps: 0.214 | LR: 0.000100 | Steps: 2063797\n",
      "Episode 3420 | Score: 210.0 | Avg: 345.95 | Eps: 0.213 | LR: 0.000100 | Steps: 2071176\n",
      "Episode 3430 | Score: 210.0 | Avg: 339.50 | Eps: 0.213 | LR: 0.000100 | Steps: 2077913\n",
      "Episode 3440 | Score: 285.0 | Avg: 324.20 | Eps: 0.212 | LR: 0.000100 | Steps: 2085069\n",
      "\t\tRAM: 83.4% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 3450 | Score: 285.0 | Avg: 329.55 | Eps: 0.211 | LR: 0.000100 | Steps: 2092277\n",
      "Episode 3460 | Score: 510.0 | Avg: 326.80 | Eps: 0.210 | LR: 0.000100 | Steps: 2098702\n",
      "Episode 3470 | Score: 215.0 | Avg: 319.60 | Eps: 0.210 | LR: 0.000100 | Steps: 2105944\n",
      "Episode 3480 | Score: 360.0 | Avg: 321.55 | Eps: 0.209 | LR: 0.000100 | Steps: 2113151\n",
      "\t\tRAM: 82.7% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 3490 | Score: 180.0 | Avg: 317.50 | Eps: 0.208 | LR: 0.000100 | Steps: 2120337\n",
      "Episode 3500 | Score: 245.0 | Avg: 314.85 | Eps: 0.207 | LR: 0.000100 | Steps: 2126821\n",
      "Episode 3510 | Score: 275.0 | Avg: 319.65 | Eps: 0.206 | LR: 0.000100 | Steps: 2134285\n",
      "Episode 3520 | Score: 530.0 | Avg: 306.70 | Eps: 0.206 | LR: 0.000100 | Steps: 2140187\n",
      "\t\tRAM: 83.1% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 3530 | Score: 210.0 | Avg: 313.05 | Eps: 0.205 | LR: 0.000100 | Steps: 2146748\n",
      "Episode 3540 | Score: 390.0 | Avg: 314.45 | Eps: 0.205 | LR: 0.000100 | Steps: 2153146\n",
      "Episode 3550 | Score: 210.0 | Avg: 305.45 | Eps: 0.204 | LR: 0.000100 | Steps: 2159448\n",
      "Episode 3560 | Score: 210.0 | Avg: 321.30 | Eps: 0.203 | LR: 0.000100 | Steps: 2167266\n",
      "\t\tRAM: 83.1% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 3570 | Score: 445.0 | Avg: 333.40 | Eps: 0.202 | LR: 0.000100 | Steps: 2175022\n",
      "Episode 3580 | Score: 510.0 | Avg: 331.65 | Eps: 0.202 | LR: 0.000100 | Steps: 2181454\n",
      "Episode 3590 | Score: 465.0 | Avg: 328.70 | Eps: 0.201 | LR: 0.000100 | Steps: 2187227\n",
      "Episode 3600 | Score: 550.0 | Avg: 334.70 | Eps: 0.200 | LR: 0.000100 | Steps: 2194201\n",
      "\t\tRAM: 83.4% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 3610 | Score: 225.0 | Avg: 334.55 | Eps: 0.200 | LR: 0.000100 | Steps: 2201080\n",
      "Episode 3620 | Score: 260.0 | Avg: 343.30 | Eps: 0.199 | LR: 0.000100 | Steps: 2207817\n",
      "Episode 3630 | Score: 545.0 | Avg: 347.00 | Eps: 0.198 | LR: 0.000100 | Steps: 2214827\n",
      "Episode 3640 | Score: 215.0 | Avg: 341.00 | Eps: 0.198 | LR: 0.000100 | Steps: 2220648\n",
      "\t\tRAM: 82.6% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 3650 | Score: 470.0 | Avg: 337.95 | Eps: 0.197 | LR: 0.000100 | Steps: 2226404\n",
      "Episode 3660 | Score: 195.0 | Avg: 329.15 | Eps: 0.196 | LR: 0.000100 | Steps: 2233337\n",
      "Episode 3670 | Score: 340.0 | Avg: 314.05 | Eps: 0.196 | LR: 0.000100 | Steps: 2240020\n",
      "Episode 3680 | Score: 440.0 | Avg: 314.55 | Eps: 0.195 | LR: 0.000100 | Steps: 2246867\n",
      "\t\tRAM: 83.3% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 3690 | Score: 200.0 | Avg: 321.60 | Eps: 0.194 | LR: 0.000100 | Steps: 2254911\n",
      "Episode 3700 | Score: 595.0 | Avg: 323.25 | Eps: 0.194 | LR: 0.000100 | Steps: 2262179\n",
      "Episode 3710 | Score: 315.0 | Avg: 316.90 | Eps: 0.193 | LR: 0.000100 | Steps: 2269302\n",
      "Episode 3720 | Score: 460.0 | Avg: 314.15 | Eps: 0.192 | LR: 0.000100 | Steps: 2276364\n",
      "\t\tRAM: 83.0% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 3730 | Score: 400.0 | Avg: 307.90 | Eps: 0.192 | LR: 0.000100 | Steps: 2282904\n",
      "Episode 3740 | Score: 425.0 | Avg: 314.20 | Eps: 0.191 | LR: 0.000100 | Steps: 2290727\n",
      "Episode 3750 | Score: 395.0 | Avg: 322.15 | Eps: 0.190 | LR: 0.000100 | Steps: 2298424\n",
      "Episode 3760 | Score: 315.0 | Avg: 315.70 | Eps: 0.190 | LR: 0.000100 | Steps: 2304524\n",
      "\t\tRAM: 83.3% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 3770 | Score: 265.0 | Avg: 314.20 | Eps: 0.189 | LR: 0.000100 | Steps: 2310895\n",
      "Episode 3780 | Score: 210.0 | Avg: 308.55 | Eps: 0.189 | LR: 0.000100 | Steps: 2317274\n",
      "Episode 3790 | Score: 335.0 | Avg: 298.60 | Eps: 0.188 | LR: 0.000100 | Steps: 2322952\n",
      "Episode 3800 | Score: 180.0 | Avg: 297.05 | Eps: 0.188 | LR: 0.000100 | Steps: 2330115\n",
      "\t\tRAM: 82.9% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 3810 | Score: 455.0 | Avg: 293.75 | Eps: 0.187 | LR: 0.000100 | Steps: 2336557\n",
      "Episode 3820 | Score: 215.0 | Avg: 285.35 | Eps: 0.187 | LR: 0.000100 | Steps: 2341745\n",
      "Episode 3830 | Score: 280.0 | Avg: 281.35 | Eps: 0.186 | LR: 0.000100 | Steps: 2347332\n",
      "Episode 3840 | Score: 265.0 | Avg: 278.60 | Eps: 0.185 | LR: 0.000100 | Steps: 2353968\n",
      "\t\tRAM: 83.8% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 3850 | Score: 255.0 | Avg: 272.60 | Eps: 0.185 | LR: 0.000100 | Steps: 2360399\n",
      "Episode 3860 | Score: 440.0 | Avg: 286.80 | Eps: 0.184 | LR: 0.000100 | Steps: 2367447\n",
      "Episode 3870 | Score: 430.0 | Avg: 292.05 | Eps: 0.184 | LR: 0.000100 | Steps: 2374094\n",
      "Episode 3880 | Score: 260.0 | Avg: 292.20 | Eps: 0.183 | LR: 0.000100 | Steps: 2380712\n",
      "\t\tRAM: 81.9% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 3890 | Score: 210.0 | Avg: 301.50 | Eps: 0.183 | LR: 0.000100 | Steps: 2388172\n",
      "Episode 3900 | Score: 530.0 | Avg: 310.25 | Eps: 0.182 | LR: 0.000100 | Steps: 2395674\n",
      "Episode 3910 | Score: 155.0 | Avg: 308.75 | Eps: 0.182 | LR: 0.000100 | Steps: 2401727\n",
      "Episode 3920 | Score: 265.0 | Avg: 318.80 | Eps: 0.181 | LR: 0.000100 | Steps: 2409252\n",
      "\t\tRAM: 81.4% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 3930 | Score: 320.0 | Avg: 326.35 | Eps: 0.180 | LR: 0.000100 | Steps: 2415789\n",
      "Episode 3940 | Score: 210.0 | Avg: 328.35 | Eps: 0.180 | LR: 0.000100 | Steps: 2422023\n",
      "Episode 3950 | Score: 465.0 | Avg: 329.25 | Eps: 0.179 | LR: 0.000100 | Steps: 2428932\n",
      "Episode 3960 | Score: 245.0 | Avg: 323.25 | Eps: 0.179 | LR: 0.000100 | Steps: 2435762\n",
      "\t\tRAM: 82.8% | GPU: Active | Buffer: 150000/150000\n",
      "Episode 3970 | Score: 225.0 | Avg: 322.00 | Eps: 0.178 | LR: 0.000100 | Steps: 2442455\n"
     ]
    }
   ],
   "source": [
    "rewards_ddqn = train_agent(CONFIG_DDQN, agent_ddqn, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Duelling DQN agent\n",
    "\n",
    "CONFIG_DuelDQN = BASE_CONFIG.copy()\n",
    "CONFIG_DuelDQN['DQN_TYPE'] = 'DuellingDQN'\n",
    "CONFIG_DuelDQN['USE_PER'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_dueldqn = DQNAgent(CONFIG_DuelDQN, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_dueldqn = train_agent(CONFIG_DuelDQN, agent_dueldqn, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rewards(rewards, window=100):\n",
    "    \"\"\"Plot training rewards with moving average.\"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(rewards, alpha=0.6, label='Episode Reward')\n",
    "    \n",
    "    if len(rewards) >= window:\n",
    "        moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "        plt.plot(range(window-1, len(rewards)), moving_avg, \n",
    "                label=f'Moving Avg ({window})', linewidth=2, color='red')\n",
    "    \n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Training Progress')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_rewards(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 'rewards' is the list returned by your train_dqn function\n",
    "# If you used the variable name 'rewards_ddqn_per', replace 'rewards' below with that.\n",
    "\n",
    "# 1. Highest Single Episode Score (The \"High Score\")\n",
    "max_score = max(rewards)\n",
    "max_episode = np.argmax(rewards)\n",
    "print(f\"üöÄ Highest Single Episode Score: {max_score} (Achieved at Episode {max_episode})\")\n",
    "\n",
    "# 2. Best Average Score (The most stable policy)\n",
    "# Calculate moving average over 100 episodes\n",
    "window = 100\n",
    "if len(rewards) >= window:\n",
    "    moving_avgs = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    best_avg_score = np.max(moving_avgs)\n",
    "    best_avg_episode = np.argmax(moving_avgs) + window # Adjustment for window offset\n",
    "    print(f\"üèÜ Best 100-Episode Average: {best_avg_score:.2f} (Achieved around Episode {best_avg_episode})\")\n",
    "else:\n",
    "    print(\"Not enough episodes for 100-episode average.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(rewards, alpha=0.4, color='gray', label='Episode Reward') # Faint raw scores\n",
    "if len(rewards) >= 100:\n",
    "    plt.plot(range(99, len(rewards)), moving_avgs, color='blue', linewidth=2, label='100-Ep Moving Avg')\n",
    "    # Mark the best point\n",
    "    plt.scatter(best_avg_episode-100, best_avg_score, color='red', s=100, zorder=5, label='Best Average')\n",
    "\n",
    "plt.axhline(y=500, color='green', linestyle='--', label='Target: 500')\n",
    "plt.title(\"Training Progress: Double DQN + PER\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watch a Smart Agent!\n",
    "\n",
    "In the next code cell, you will load the trained weights from file to watch a smart agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weights from file\n",
    "agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used code from https://www.anyscale.com/blog/an-introduction-to-reinforcement-learning-with-openai-gym-rllib-and-google\n",
    "# for the video saving and display\n",
    "\n",
    "before_training = \"trained.mp4\"\n",
    "\n",
    "video = VideoRecorder(env, before_training)\n",
    "# returns an initial observation\n",
    "state = env.reset()\n",
    "score = 0\n",
    "for i in range(1000):\n",
    "  env.render()\n",
    "  video.capture_frame()\n",
    "  # env.action_space.sample() produces either 0 (left) or 1 (right).\n",
    "  action = agent.act(state)\n",
    "  state, reward, done, _ = env.step(action)\n",
    "  score += reward\n",
    "  if done:\n",
    "    break\n",
    "\n",
    "video.close()\n",
    "env.close()\n",
    "\n",
    "print(\"Total score was\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from base64 import b64encode\n",
    "def render_mp4(videopath: str) -> str:\n",
    "  \"\"\"\n",
    "  Gets a string containing a b4-encoded version of the MP4 video\n",
    "  at the specified path.\n",
    "  \"\"\"\n",
    "  mp4 = open(videopath, 'rb').read()\n",
    "  base64_encoded_mp4 = b64encode(mp4).decode()\n",
    "  return f'<video width=400 controls><source src=\"data:video/mp4;' \\\n",
    "         f'base64,{base64_encoded_mp4}\" type=\"video/mp4\"></video>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "html = render_mp4(before_training)\n",
    "HTML(html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_msc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
